{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3fc89c-f9b7-4623-a0b6-37540842568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from pyinfusion import TransitionDynamic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "#import torch\n",
    "#from collections import deque\n",
    "#import random\n",
    "\n",
    "#import QPLEXBuild\n",
    "#import SingleAgentRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153a12c-a65f-464e-8b38-276da8ef7639",
   "metadata": {},
   "source": [
    "## Preparing the Data based on Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79b0151-121a-4649-bb43-181e89b5b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(infusion_data, maintenance_data, num_actions = 2):\n",
    "    \n",
    "    ## insert repair label\n",
    "    maintenance_data['repair'] = np.where(maintenance_data.PartCost.notnull(), 1, 0)\n",
    "\n",
    "    # \n",
    "    maint_df = maintenance_data.copy()\n",
    "\n",
    "    ## group the maintenance df\n",
    "    maint_df = maint_df.groupby('WO_WO#').agg({'repair': 'first'}).reset_index()\n",
    "\n",
    "    ## left join the infusion data and maintenance data\n",
    "    infusion_data = infusion_data.merge(maint_df, how='left',\n",
    "                                       left_on = 'WO_WO#', right_on = 'WO_WO#')\n",
    "\n",
    "    ## split the infusion_data into scenarios of repair and non_repair\n",
    "    split_data = {}\n",
    "\n",
    "    for i in range(num_actions):\n",
    "        split_data[i] = {'infusion_log' : infusion_data[infusion_data.repair == i],\n",
    "                         'maintenance_log' : maintenance_data[maintenance_data.repair == i]\n",
    "                        }\n",
    "\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc72c5-9c20-4927-add5-8488c33edcef",
   "metadata": {},
   "source": [
    "# Single Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d5981-3c5b-4962-b4c2-1d0b39f373f7",
   "metadata": {},
   "source": [
    "## Computing Transition Matrix - Single Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfd248d-d313-4ef4-ade5-345ba551355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_transition_matrix(prepped_failure_info, num_actions, states, col_name = 'PCUSerialNumber'):\n",
    "    trans_matrices = {}\n",
    "\n",
    "    for i in range(num_actions):\n",
    "        action_infusion_information = prepped_failure_info[i]['infusion_log']\n",
    "        action_maint_information = prepped_failure_info[i]['maintenance_log']\n",
    "\n",
    "        # instantitate the Transition Dynamic\n",
    "        transition_dynamic = TransitionDynamic(infusion_log = action_infusion_information,\n",
    "                                               maintenance_log = action_maint_information,\n",
    "                                               col_name = col_name,\n",
    "                                               states = states)\n",
    "\n",
    "        # calculate the the DTMC matrix\n",
    "        trans_matrices[i] = transition_dynamic.system_environment(group_assets=True)\n",
    "\n",
    "    return trans_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155b08b-db98-4f7c-a655-c5b9eeb9ac87",
   "metadata": {},
   "source": [
    "# Multi Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d2967-84ac-43c2-a629-396780386061",
   "metadata": {},
   "source": [
    "## Compute Transition Matrix - Multi Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729db249-2563-4ebb-8058-79be18ac3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_agent(transition_matrix_, agents_id, num_actions):\n",
    "    agents_trans_env = [] # store agents transition information\n",
    "    transition_evironment = {} # store the transition matrix for each action\n",
    "\n",
    "    for agent in agents_id:\n",
    "        transition_environment = {}\n",
    "\n",
    "        for action in range(num_actions):\n",
    "            transition_environment[action] = transition_matrix_[action][agent]['dtmc_matrix_pandas'].to_json(orient='records')\n",
    "\n",
    "        agents_trans_env.append(transition_environment)\n",
    "\n",
    "    return agents_trans_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b946c-3dad-4688-b677-01753dc575ad",
   "metadata": {},
   "source": [
    "## Compute Cost - Multi Agent\n",
    "- Assume Fixed Cost for Transition State based on Generalized Cost Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a240832-de3b-4f2e-9eed-bf728d8e1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_multi_agent(costs_single, num_agents):\n",
    "    return [costs_single for agent in range(num_agents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff836189-bf7e-45d1-be10-4dceb3c60ad3",
   "metadata": {},
   "source": [
    "# Obtain Agent Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881a62dd-68a4-41ac-925a-19d5475e3766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCUSerialNumber</th>\n",
       "      <th>WO_Requested</th>\n",
       "      <th>WO_WO#</th>\n",
       "      <th>WO_Type</th>\n",
       "      <th>ActiveStartTime</th>\n",
       "      <th>ActiveStopTime</th>\n",
       "      <th>TotalInfusionTime</th>\n",
       "      <th>TotalEqActiveTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12828160</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>682305</td>\n",
       "      <td>CEIN</td>\n",
       "      <td>2020-01-02 18:14:58</td>\n",
       "      <td>2020-08-15 09:07:06</td>\n",
       "      <td>40707750</td>\n",
       "      <td>52291662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12828160</td>\n",
       "      <td>2021-08-19</td>\n",
       "      <td>742583</td>\n",
       "      <td>CEIN</td>\n",
       "      <td>2020-08-18 22:37:09</td>\n",
       "      <td>2020-12-31 20:39:32</td>\n",
       "      <td>27605763</td>\n",
       "      <td>32650026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PCUSerialNumber WO_Requested  WO_WO# WO_Type      ActiveStartTime  \\\n",
       "0         12828160   2020-08-19  682305    CEIN  2020-01-02 18:14:58   \n",
       "1         12828160   2021-08-19  742583    CEIN  2020-08-18 22:37:09   \n",
       "\n",
       "        ActiveStopTime  TotalInfusionTime  TotalEqActiveTime  \n",
       "0  2020-08-15 09:07:06           40707750           52291662  \n",
       "1  2020-12-31 20:39:32           27605763           32650026  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the data\n",
    "pcu_failure_info = pd.read_csv('/Users/mobolajishobanke/Desktop/Fall Research/NN_Class_Project/pcu_failure_information.csv')\n",
    "\n",
    "## check sample\n",
    "pcu_failure_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9d3369-7a56-4f13-9ae0-2aa2f163b914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Asset_Status', 'Asset_AssetID', 'Asset_Serial', 'Asset_AssetPK',\n",
       "       'Asset_Classification', 'Asset_Model', 'Asset_Manufacturer',\n",
       "       'Asset_InstallDate', 'WO_WO#', 'WO_Requested', 'WO_Closed', 'WO_Type',\n",
       "       'WO_Type_Desc', 'WO_Substatus', 'WO_Problem', 'WO_Failure',\n",
       "       'WO_Solution', 'PartID', 'PartName', 'PartCost', 'WO_Reason',\n",
       "       'WO_LaborReport'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading in the maintenance data\n",
    "maintenance_data = pd.read_excel('/Users/mobolajishobanke/Desktop/Fall Research/NN_Class_Project/maintenance_data_2005_2022.xlsx')\n",
    "\n",
    "## filter based on CEIN, CECM, HZARD\n",
    "maintenance_data = maintenance_data[maintenance_data.WO_Type.isin(['CECM', 'CEIN', 'HZARC'])]\n",
    "\n",
    "maintenance_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6cdd83a-7fd9-4b20-bb04-8ccc9dc1b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1])\n",
      "dict_keys(['infusion_log', 'maintenance_log'])\n"
     ]
    }
   ],
   "source": [
    "# compute prepped data from infusion and maintenance data\n",
    "failure_info = prepare_data(infusion_data = pcu_failure_info,\n",
    "                           maintenance_data = maintenance_data)\n",
    "\n",
    "print(failure_info.keys())\n",
    "print(failure_info[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179c3f56-cf96-409f-b8b3-26f19d817698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PartCost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WO_Type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CECM</th>\n",
       "      <td>60.731572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEIN</th>\n",
       "      <td>28.667771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HZARC</th>\n",
       "      <td>18.190441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PartCost\n",
       "WO_Type           \n",
       "CECM     60.731572\n",
       "CEIN     28.667771\n",
       "HZARC    18.190441"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## obtain maintenance cost per state\n",
    "maint_costs =  maintenance_data.groupby('WO_Type').agg({'PartCost':'mean'})\n",
    "maint_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51d2bc52-5f7c-427f-a0e7-6e843c7c52e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['infusing', 'CEIN', 'CECM', 'HZARC']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all states in the system\n",
    "states_ = ['infusing'] + maintenance_data.WO_Type.unique().tolist()\n",
    "states_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b446fa-0a8b-4ded-affe-29b1dacbf4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cost for all states. It is assumed that it is more expensive to not carry out a repair if a system breaks down\n",
    "costs = {\n",
    "    0:{0: 0.0, 1: 0, 2: 160.73, 3: 118.19},\n",
    "    1: {0: 0.0, 1: 28.67, 2: 60.73, 3: 18.19}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a071a9dc-f55d-4541-8aa8-ed7e73c4ca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transtion Matrix Computation: 100%|██████████| 1016/1016 [00:02<00:00, 376.38it/s]\n",
      "Transtion Matrix Computation: 100%|██████████| 1381/1381 [00:03<00:00, 435.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute the action transition matrices for all agents\n",
    "transition_matrices_all_agents = action_transition_matrix(prepped_failure_info = failure_info, \n",
    "                                                         num_actions = 2,\n",
    "                                                         states = states_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528731c0-ba81-4703-a5df-8ed322b0ab41",
   "metadata": {},
   "source": [
    "### Selecting Sample Agent To Be Used ValueIteration and DQN Implementation\n",
    " - use agent with the most number of occurrence in the infusion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c18f87f0-fd3d-42df-86c0-27c0cd4a07da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCUSerialNumber</th>\n",
       "      <th>ActiveStartTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12992579</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13923991</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13923356</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14154795</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14157251</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PCUSerialNumber  ActiveStartTime\n",
       "0         12992579                7\n",
       "1         13923991                6\n",
       "2         13923356                6\n",
       "3         14154795                5\n",
       "4         14157251                5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrence_count = pcu_failure_info.groupby('PCUSerialNumber').agg({'ActiveStartTime': 'count'}).sort_values(by= 'ActiveStartTime', ascending = False).reset_index()\n",
    "agents_serial = occurrence_count.PCUSerialNumber.values.tolist()[:5]\n",
    "occurrence_count.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fdaa98c-79f4-469f-8452-0626bfb2cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12992579, 13923991, 13923356, 14154795, 14157251]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view agents serial\n",
    "agents_serial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610e4b7-c6ed-4ea0-aa87-f54ae6d24523",
   "metadata": {},
   "source": [
    "### Obtain Transition Matrix and Cost of Selected Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adf5d3db-d8d2-467b-8ff9-828f5469b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0:\n",
      "[{\"0.0\":0.0,\"1.0\":0.3333333333,\"2.0\":0.6666666667},{\"0.0\":0.0,\"1.0\":0.0,\"2.0\":1.0},{\"0.0\":0.6666666667,\"1.0\":0.0,\"2.0\":0.3333333333}] \n",
      "\n",
      "action: 1:\n",
      "[{\"0.0\":0.0,\"1.0\":0.0,\"2.0\":1.0},{\"0.0\":1.0,\"1.0\":0.0,\"2.0\":0.0},{\"0.0\":0.2,\"1.0\":0.4,\"2.0\":0.4}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chose agent\n",
    "chosen_agent = agents_serial[:1]\n",
    "\n",
    "agent_transition_matrix = select_agent(transition_matrix_ = transition_matrices_all_agents,\n",
    "                                      agents_id = chosen_agent,\n",
    "                                      num_actions = 2)\n",
    "\n",
    "# print transition matrices for each action\n",
    "num_actions = 2\n",
    "\n",
    "for agent in range(len(chosen_agent)):\n",
    "    for action in range(num_actions):\n",
    "        print(f'action: {action}:\\n{agent_transition_matrix[agent][action]} \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082ba21-76d2-4663-b74f-5fbe679db2f9",
   "metadata": {},
   "source": [
    "### Store Agent Interation Information in a json file for future import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97956758-65e9-45ef-b843-a8c7526d358e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.0       1.0       2.0\n",
       "0  0.000000  0.333333  0.666667\n",
       "1  0.000000  0.000000  1.000000\n",
       "2  0.666667  0.000000  0.333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(json.loads(agent_transition_matrix[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84b1bcb8-7221-4b9f-b7b4-4851b12f46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store information\n",
    "#agent_information = (chosen_agent, agent_transition_matrix, costs)\n",
    "agent_information = (agent_transition_matrix, costs)\n",
    "\n",
    "# serializing infrmation for json\n",
    "agent_information = json.dumps(agent_information)\n",
    "\n",
    "with open('single_agent_historical_information.json', 'w') as file:\n",
    "    file.write(agent_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24354404-e4a3-42de-93d3-b3f0a1a5ceae",
   "metadata": {},
   "source": [
    "### Selecting Multiple Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e25921d6-0a5c-43bb-aa36-149d8dfe9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent ID: 12992579\n",
      "action: 0:\n",
      "[{\"0.0\":0.0,\"1.0\":0.3333333333,\"2.0\":0.6666666667},{\"0.0\":0.0,\"1.0\":0.0,\"2.0\":1.0},{\"0.0\":0.6666666667,\"1.0\":0.0,\"2.0\":0.3333333333}] \n",
      "\n",
      "action: 1:\n",
      "[{\"0.0\":0.0,\"1.0\":0.0,\"2.0\":1.0},{\"0.0\":1.0,\"1.0\":0.0,\"2.0\":0.0},{\"0.0\":0.2,\"1.0\":0.4,\"2.0\":0.4}] \n",
      "\n",
      "Agent ID: 13923991\n",
      "action: 0:\n",
      "[{\"0.0\":0.0,\"1.0\":0.25,\"2.0\":0.75},{\"0.0\":1.0,\"1.0\":0.0,\"2.0\":0.0},{\"0.0\":0.4,\"1.0\":0.2,\"2.0\":0.4}] \n",
      "\n",
      "action: 1:\n",
      "[{\"0.0\":0.0,\"1.0\":0.5,\"2.0\":0.5},{\"0.0\":1.0,\"1.0\":0.0,\"2.0\":0.0},{\"0.0\":0.0,\"1.0\":1.0,\"2.0\":0.0}] \n",
      "\n",
      "Agent ID: 13923356\n",
      "action: 0:\n",
      "[{\"0.0\":0.0,\"2.0\":1.0},{\"0.0\":1.0,\"2.0\":0.0}] \n",
      "\n",
      "action: 1:\n",
      "[{\"0.0\":0.0,\"1.0\":0.5,\"2.0\":0.5},{\"0.0\":0.0,\"1.0\":0.0,\"2.0\":1.0},{\"0.0\":0.6,\"1.0\":0.2,\"2.0\":0.2}] \n",
      "\n",
      "Agent ID: 14154795\n",
      "action: 0:\n",
      "[{\"0.0\":0.0,\"1.0\":0.3333333333,\"2.0\":0.6666666667},{\"0.0\":0.0,\"1.0\":0.0,\"2.0\":1.0},{\"0.0\":0.6666666667,\"1.0\":0.0,\"2.0\":0.3333333333}] \n",
      "\n",
      "action: 1:\n",
      "[{\"0.0\":0.0,\"1.0\":0.5,\"2.0\":0.5},{\"0.0\":1.0,\"1.0\":0.0,\"2.0\":0.0},{\"0.0\":0.0,\"1.0\":1.0,\"2.0\":0.0}] \n",
      "\n",
      "Agent ID: 14157251\n",
      "action: 0:\n",
      "[{\"0.0\":0.0,\"1.0\":0.5,\"2.0\":0.5},{\"0.0\":0.5,\"1.0\":0.5,\"2.0\":0.0}] \n",
      "\n",
      "action: 1:\n",
      "[{\"0.0\":0.0,\"1.0\":0.3333333333,\"2.0\":0.6666666667},{\"0.0\":1.0,\"1.0\":0.0,\"2.0\":0.0},{\"0.0\":0.0,\"1.0\":0.6666666667,\"2.0\":0.3333333333}] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chose agent\n",
    "chosen_agent = agents_serial[:5]\n",
    "\n",
    "agent_transition_matrix = select_agent(transition_matrix_ = transition_matrices_all_agents,\n",
    "                                      agents_id = chosen_agent,\n",
    "                                      num_actions = 2)\n",
    "\n",
    "# print transition matrices for each action\n",
    "num_actions = 2\n",
    "\n",
    "for agent in range(len(chosen_agent)):\n",
    "    print('Agent ID: {}'.format(chosen_agent[agent]) )\n",
    "    for action in range(num_actions):\n",
    "        print(f'action: {action}:\\n{agent_transition_matrix[agent][action]} \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61901dff-c523-4e55-b50b-efb937e73674",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiagent_cost = cost_multi_agent(costs_single =costs,\n",
    "                                   num_agents = len(chosen_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c188fb9b-9622-41e8-a064-2f607074fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_agent_idx = [0, 1, 3]\n",
    "\n",
    "selected_multiagent_choice = [agent_transition_matrix[i] for i in idx_agent_idx]\n",
    "costs_selected = [multiagent_cost[i] for i in idx_agent_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfa7e736-b665-4dc3-ab2d-e190ff8f0cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0.0': 0.0, '1.0': 0.5, '2.0': 0.5},\n",
       " {'0.0': 1.0, '1.0': 0.0, '2.0': 0.0},\n",
       " {'0.0': 0.0, '1.0': 1.0, '2.0': 0.0}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(agent_transition_matrix[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e475207-9f3e-4ef3-a055-38593fe6fb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ebe4f67-5028-481a-a36f-f41842b19595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0, 1: 28.67, 2: 60.73, 3: 18.19}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiagent_cost[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f271ae1-b13e-4423-b803-7752bbefc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # store information\n",
    "agent_information = (selected_multiagent_choice, costs_selected)\n",
    "\n",
    "# serializing infrmation for json\n",
    "agent_information = json.dumps(agent_information)\n",
    "\n",
    "with open('multi_agent_3___new.json', 'w') as file:\n",
    "    file.write(agent_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec89f66-b338-4f75-8010-2a7772f067c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e3067-9ad5-4835-8321-32605241bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change pandas dataframe to json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dd364-87fe-4c6d-b9a9-6d748dd886e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_information = (chosen_agent, agent_transition_matrix, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cb2b6-4a29-489d-b748-064022f759bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the agent information as a json file\n",
    "with open('single_agent_information.json', 'w') as file:\n",
    "    json.dump(agent_information, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6d7b0-f8b5-448b-a63c-fccaf0126b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd8953-8d05-48a0-a96f-3fe0a319cdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd97cb-4763-49c5-85b2-4af7d62554bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c134b-d9f1-4be2-a27b-7611bc4e76ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a476a9f-b00b-41eb-838a-5d9de6c7c619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e639d8-b461-41fc-9ba4-8f1c7711417a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e236d-61d2-4185-92c2-339a905fe173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba271b-819a-4a0e-adf1-c3788322584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## maintenance_dat\n",
    "maint_costs =  maintenance_data.groupby('WO_Type').agg({'PartCost':'mean'})\n",
    "maint_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c075f5-914b-4adc-a204-e028fa14648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = {\n",
    "    0:{0: 0.0, 1: 0, 2: 160.73, 3: 118.19},\n",
    "    1: {0: 0.0, 1: 28.67, 2: 60.73, 3: 18.19}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629eab52-4bea-4e97-8c9b-d3c8662b5e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c2954-704d-47cb-9b43-d85e468231ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62d4f4-edde-42ae-9d72-3c8a8fbf2946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765c30e-5e3a-41e6-9ccd-b389d7507e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac1231-4708-4bd5-aa61-bf7253a5e38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fbf8d-2bb1-41ed-8c25-d6268c2a7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_compute(maint_cost, encoded_states, num_actions, states):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    num_states = len(states)\n",
    "    \n",
    "    action_costs = {} # costs per action\n",
    "    costs = {}\n",
    "\n",
    "    for action in range(num_actions):\n",
    "        for state in range(num_state):\n",
    "            if states[state] == 0:\n",
    "                costs[state] = 0\n",
    "\n",
    "            if action == 0 and states[state] == 'CEIN': # CEIN : Planned Maintenance\n",
    "                costs[state] = 0\n",
    "                \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497b4bd-2388-4fb6-bc63-cb97ee495750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d26400-b4a4-41d0-810e-e9762c875f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6698e-ca87-4d1f-8d1d-851e9cb98d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a0ad3-e05b-4c6f-a024-27c7c33c8645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5507d28-9f22-4e84-b762-c2477d4306b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_ = ['infu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f74512-1fad-4c2a-abb4-a70b3035ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = 0\n",
    "infusion_0 = failure_info[0]['infusion_log']\n",
    "maint_0 = failure_info[0]['maintenance_log']\n",
    "\n",
    "# action = 1\n",
    "infusion_1 = failure_info[1]['infusion_log']\n",
    "maint_1 = failure_info[1]['maintenance_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b194b-f3e7-4144-a35c-c892e386a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_types = maintenance_data.WO_Type.unique().tolist()\n",
    "states_ = ['infusing'] + wo_types\n",
    "states_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dceb7a4-d6de-4e9d-a960-57bb5368e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = 0\n",
    "transition_dynamic_0 = TransitionDynamic(infusion_log = infusion_0,\n",
    "                                        maintenance_log = maint_0,\n",
    "                                        col_name = 'PCUSerialNumber',\n",
    "                                        states = states_)\n",
    "\n",
    "## obtain the dtmc transition information\n",
    "transition_matrix_0 = transition_dynamic_0.system_environment(group_assets = True)\n",
    "\n",
    "# action = 1\n",
    "transition_dynamic_1 = TransitionDynamic(infusion_log = infusion_1,\n",
    "                                        maintenance_log = maint_1,\n",
    "                                        col_name = 'PCUSerialNumber',\n",
    "                                        states = states_)\n",
    "\n",
    "## obtain the dtmc transition information\n",
    "transition_matrix_1 = transition_dynamic_1.system_environment(group_assets = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbea012-d144-4c33-8512-be27a74918ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcu_failure_info.groupby('PCUSerialNumber').agg({'ActiveStartTime': 'count'}).sort_values(by= 'ActiveStartTime', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41a599-ca6b-4fc9-a02b-60ecdd265772",
   "metadata": {},
   "source": [
    "## Transition Dynamic for Sample Agent/PCU = 12992579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec452f-711a-4dcf-ae0e-94e9490d70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting information for sample pcu\n",
    "transition_matrix_0[12992579].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76add27a-f8f7-4838-abcb-2b9a1c06113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_environment = {\n",
    "    0 : transition_matrix_0[12992579]['dtmc_matrix_pandas'],\n",
    "    1: transition_matrix_1[12992579]['dtmc_matrix_pandas']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e85c5-8e61-4bb4-aaaf-9f0182a653c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_environment[0].loc[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabc7f5-e878-4510-a909-f0b1110ada37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## maintenance_dat\n",
    "maint_costs =  maintenance_data.groupby('WO_Type').agg({'PartCost':'mean'})\n",
    "maint_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e691b-d7e2-4e14-a6d0-e9ddec7703c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bebfae-2f32-4d68-b490-2a06badac2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635537e-e288-48e7-b283-67a1b7a65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = [0]\n",
    "\n",
    "for i in states_[1:]:\n",
    "    cost_i = round(maint_1[maint_1.WO_Type == i].PartCost.mean(), 2)\n",
    "    c1.append(cost_i)\n",
    "\n",
    "c0 = [0, 0, c1[2]+100]\n",
    "\n",
    "print('c0 : {}'.format(c0))\n",
    "print('c1 : {}'.format(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf582221-5c1f-4f23-8eb4-2832b556a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_environment = {\n",
    "    0 : transition_matrix_0[12992579]['dtmc_matrix'],\n",
    "    1: transition_matrix_1[12992579]['dtmc_matrix']\n",
    "}\n",
    "\n",
    "cost = {0 : c0, 1 : c1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612a3c6-aac4-4fee-9122-0a94f9347fad",
   "metadata": {},
   "source": [
    "## Value Determination for Sample Agent - Run With Pyinfusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5361a1-ed0a-4790-870e-2087b6d5af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the optimal policy using the Value Iteration Formula\n",
    "\n",
    "optimal_policy = pyinfusion.value_iteration(environment = transition_environment,\n",
    "                                               cost = cost,\n",
    "                                               n_actions = 2,\n",
    "                                               tol = 1e-16,\n",
    "                                               states = states_)\n",
    "\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86180053-ee3c-410d-8b19-627bab4ff4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379d91c-568d-4167-9209-ef45dff7ea55",
   "metadata": {},
   "source": [
    "## Testing Online DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36a73d-a6de-463c-a577-5454f7cdfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = pyinfusion.OnlineDQN(environment = transition_environment,\n",
    "                          cost = cost,\n",
    "                          num_states = len(states_),\n",
    "                          num_actions = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a5a8a-ec4a-4099-b43e-673a2badea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94809742-a357-40e3-8ab2-f68685cd9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = dqn.q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71eb55-27ef-4e87-9cf1-356d32bd1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(states_)\n",
    "data = np.eye(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b1bd3-5806-407e-a9c1-ab80c544a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f5b25-7ad4-4c40-a9ec-e08fa0e8ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(mod.predict(data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d04cb-45aa-4c7d-80cd-9399d21751f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xer = pyinfusion.OfflineDQN(environment = transition_environment,\n",
    "                          cost = cost,\n",
    "                          num_states = len(states_),\n",
    "                          num_actions = 2)\n",
    "\n",
    "xer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9a1e4-1a4f-4ff5-9ac3-0e1e5a16311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = xer.action_predictor\n",
    "np.argmin(mod.predict(data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ce3df-b367-4be3-b8b4-bb4c19f4eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Agent DQN for Q-value estimation\n",
    "class AgentDQN(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(AgentDQN, self).__init__()\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.fc2 = layers.Dense(64, activation='relu')\n",
    "        self.q_out = layers.Dense(action_dim)  # Output Q-values\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        q_values = self.q_out(x)\n",
    "        return q_values\n",
    "\n",
    "# Transformation Network with Softplus activation to ensure positive weight outputs\n",
    "class TransformationNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(TransformationNetwork, self).__init__()\n",
    "        self.fc1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.w = layers.Dense(1, activation='softplus')  # Softplus ensures positive output\n",
    "        self.b = layers.Dense(1)\n",
    "\n",
    "    def call(self, global_state, local_value):\n",
    "        x = self.fc1(global_state)\n",
    "        x = self.fc2(x)\n",
    "        weight = self.w(x)\n",
    "        bias = self.b(x)\n",
    "        transformed_value = weight * local_value + bias\n",
    "        return transformed_value\n",
    "\n",
    "# QPLEX Model\n",
    "class QPLEX(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, num_agents, hidden_dim=64):\n",
    "        super(QPLEX, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [AgentDQN(state_dim, action_dim) for _ in range(num_agents)]\n",
    "        self.value_transformers = [TransformationNetwork(hidden_dim) for _ in range(num_agents)]\n",
    "        self.advantage_transformers = [TransformationNetwork(hidden_dim) for _ in range(num_agents)]\n",
    "        self.lambda_weights = layers.Dense(num_agents, activation='softplus')  # Positive weights\n",
    "\n",
    "    def call(self, states, global_state, actions):\n",
    "        q_values = [self.agents[i](states[:, i]) for i in range(self.num_agents)]\n",
    "        q_values = tf.stack(q_values, axis=1)  # Shape: (batch_size, num_agents, action_dim)\n",
    "\n",
    "        batch_indices = tf.range(tf.shape(q_values)[0])[:, None]\n",
    "        selected_q_values = tf.gather_nd(q_values, tf.concat([batch_indices, actions], axis=-1))\n",
    "        values = tf.reduce_max(selected_q_values, axis=-1, keepdims=True)\n",
    "        advantages = selected_q_values - values\n",
    "\n",
    "        transformed_values = [self.value_transformers[i](global_state, values[:, i]) for i in range(self.num_agents)]\n",
    "        transformed_advantages = [self.advantage_transformers[i](global_state, advantages[:, i]) for i in range(self.num_agents)]\n",
    "\n",
    "        transformed_values = tf.stack(transformed_values, axis=1)\n",
    "        transformed_advantages = tf.stack(transformed_advantages, axis=1)\n",
    "\n",
    "        lambda_weights = self.lambda_weights(global_state)\n",
    "        lambda_weights = tf.expand_dims(lambda_weights, axis=-1)\n",
    "        joint_q_value = tf.reduce_sum(transformed_values + lambda_weights * transformed_advantages, axis=1)\n",
    "        return joint_q_value  # Shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8fdc2-3c27-4797-a9be-fd5124239cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and replay buffers\n",
    "num_agents = 2\n",
    "state_dim = 4\n",
    "action_dim = 3\n",
    "transition_matrices = [np.random.rand(3, 3, 3) for _ in range(num_agents)]  # Random transition matrices\n",
    "\n",
    "env = MultiAgentEnvironment(num_agents=num_agents, transition_matrices=transition_matrices)\n",
    "buffers = [ReplayBuffer(max_size=1000) for _ in range(num_agents)]\n",
    "qplex_model = QPLEX(state_dim=state_dim, action_dim=action_dim, num_agents=num_agents)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_step(states, actions, rewards, next_states, global_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_q_values = qplex_model(states, global_state, actions)\n",
    "        target_q_values = rewards + 0.99 * qplex_model(next_states, global_state, actions)\n",
    "        loss = tf.reduce_mean(tf.square(target_q_values - current_q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, qplex_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, qplex_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(100):\n",
    "    states = env.reset()\n",
    "    for t in range(50):\n",
    "        actions = [np.random.randint(action_dim) for _ in range(num_agents)]\n",
    "        next_states, rewards = env.step(actions)\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            buffers[i].add((states[i], actions[i], rewards[i], next_states[i]))\n",
    "        \n",
    "        # Sample from buffer and train\n",
    "        if len(buffers[0]) >= 32:\n",
    "            batch_states = np.array([buffer.sample(32)[0] for buffer in buffers])\n",
    "            batch_actions = np.array([buffer.sample(32)[1] for buffer in buffers])\n",
    "            batch_rewards = np.array([buffer.sample(32)[2] for buffer in buffers])\n",
    "            batch_next_states = np.array([buffer.sample(32)[3] for buffer in buffers])\n",
    "            global_state = np.random.random((32, state_dim))  # Simplified global state\n",
    "\n",
    "            loss = train_step(batch_states, batch_actions, batch_rewards, batch_next_states, global_state)\n",
    "            print(f\"Episode {episode}, Step {t}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977564c1-3022-4247-ad16-90ddc98da7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48bc1c-c5ba-41bc-8bed-b0b9f4099ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Training step using the built-in MSE loss function\n",
    "def train_step(states, actions, rewards, next_states, global_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_q_values = qplex_model(states, global_state, actions)\n",
    "        target_q_values = rewards + 0.99 * qplex_model(next_states, global_state, actions)  # Discounted future rewards\n",
    "        \n",
    "        # Use MeanSquaredError directly\n",
    "        loss = mse_loss(target_q_values, current_q_values)\n",
    "    \n",
    "    grads = tape.gradient(loss, qplex_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, qplex_model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a10a0c-011a-475d-95ac-f3e3e4f54ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a8434-3b6f-458e-81fd-0f94defec6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_state = np.concatenate([obs for obs in agent_observations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a33d0-5e62-4ba5-8425-6d2f5b0f4696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddae5d-af1b-4530-9220-f1d894201f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set epsilon parameters\n",
    "initial_epsilon = 1.0  # Starting epsilon (full exploration)\n",
    "min_epsilon = 0.01  # Minimum epsilon (almost full exploitation)\n",
    "epsilon_decay = 0.995  # Decay rate for epsilon per episode\n",
    "\n",
    "# Initialize epsilon for each agent\n",
    "epsilon_values = [initial_epsilon for _ in range(num_agents)]\n",
    "\n",
    "def select_action(agent_index, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection for a single agent.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration: choose a random action\n",
    "        action = np.random.randint(action_dim)\n",
    "    else:\n",
    "        # Exploitation: choose the action with the highest Q-value\n",
    "        q_values = qplex_model.agents[agent_index](tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "        action = tf.argmax(q_values, axis=1).numpy()[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559cceb4-0769-4a44-b8d6-2d7604e45ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with epsilon-greedy policy for exploration\n",
    "for episode in range(total_episodes):\n",
    "    # Reset environment and initialize rewards\n",
    "    states = env.reset()\n",
    "    episode_rewards = [0 for _ in range(num_agents)]\n",
    "\n",
    "    for t in range(steps_per_episode):\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            # Select action for each agent with epsilon-greedy policy\n",
    "            action = select_action(i, states[i], epsilon_values[i])\n",
    "            actions.append(action)\n",
    "\n",
    "        # Take a step in the environment with the selected actions\n",
    "        next_states, rewards = env.step(actions)\n",
    "\n",
    "        # Accumulate rewards for tracking\n",
    "        for i in range(num_agents):\n",
    "            episode_rewards[i] += rewards[i]\n",
    "\n",
    "        # Store experiences in the replay buffer\n",
    "        for i in range(num_agents):\n",
    "            buffers[i].add((states[i], actions[i], rewards[i], next_states[i]))\n",
    "\n",
    "        # Sample from buffer and train if buffer has enough samples\n",
    "        if len(buffers[0]) >= batch_size:\n",
    "            batch_states = np.array([buffer.sample(batch_size)[0] for buffer in buffers])\n",
    "            batch_actions = np.array([buffer.sample(batch_size)[1] for buffer in buffers])\n",
    "            batch_rewards = np.array([buffer.sample(batch_size)[2] for buffer in buffers])\n",
    "            batch_next_states = np.array([buffer.sample(batch_size)[3] for buffer in buffers])\n",
    "            global_state = np.random.random((batch_size, state_dim))  # Simplified global state for training\n",
    "\n",
    "            # Train the QPLEX model\n",
    "            loss = train_step(batch_states, batch_actions, batch_rewards, batch_next_states, global_state)\n",
    "\n",
    "        # Move to next state\n",
    "        states = next_states\n",
    "\n",
    "    # Decay epsilon for each agent after the episode\n",
    "    for i in range(num_agents):\n",
    "        epsilon_values[i] = max(min_epsilon, epsilon_values[i] * epsilon_decay)\n",
    "\n",
    "    # Optionally, print the epsilon values and episode rewards\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Epsilon: {epsilon_values}, Episode Rewards: {episode_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ad528-7cbc-4f3f-879a-da8eea32049d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a839fd-23f9-4450-92bf-926bcf9bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Agent DQN for Q-value estimation\n",
    "class AgentDQN(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(AgentDQN, self).__init__()\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.fc2 = layers.Dense(64, activation='relu')\n",
    "        self.q_out = layers.Dense(action_dim)  # Output Q-values\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        q_values = self.q_out(x)\n",
    "        return q_values\n",
    "\n",
    "# Transformation Network with Softplus activation to ensure positive weights\n",
    "class TransformationNetwork(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(TransformationNetwork, self).__init__()\n",
    "        self.fc1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = layers.Dense.Dense(hidden_dim, activation='relu')\n",
    "        self.w = layers.Dense(1, activation='softplus')  # Ensures positive output for weight\n",
    "        self.b = layers.Dense(1)  # Bias term\n",
    "\n",
    "    def call(self, global_state, local_value):\n",
    "        # Use the global state as input to transform local Q-values\n",
    "        x = self.fc1(global_state)\n",
    "        x = self.fc2(x)\n",
    "        weight = self.w(x)  # Softplus ensures weight is positive\n",
    "        bias = self.b(x)\n",
    "        transformed_value = weight * local_value + bias\n",
    "        return transformed_value\n",
    "\n",
    "# QPLEX Model with Global State Concatenation\n",
    "class QPLEX(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, num_agents, hidden_dim=64):\n",
    "        super(QPLEX, self).__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [AgentDQN(state_dim, action_dim) for _ in range(num_agents)]\n",
    "        self.value_transformers = [TransformationNetwork(hidden_dim) for _ in range(num_agents)]\n",
    "        self.advantage_transformers = [TransformationNetwork(hidden_dim) for _ in range(num_agents)]\n",
    "        self.lambda_weights = layers.Dense(num_agents, activation='softplus')  # Positive weights for monotonicity\n",
    "\n",
    "    def call(self, states, actions):\n",
    "        # Concatenate all agents' states to form the global state\n",
    "        global_state = tf.concat([states[:, i] for i in range(self.num_agents)], axis=-1)  # Shape: (batch_size, total_state_dim)\n",
    "\n",
    "        # Calculate individual Q-values for each agent\n",
    "        q_values = [self.agents[i](states[:, i]) for i in range(self.num_agents)]\n",
    "        q_values = tf.stack(q_values, axis=1)  # Shape: (batch_size, num_agents, action_dim)\n",
    "\n",
    "        # Extract values and advantages for the selected actions\n",
    "        batch_indices = tf.range(tf.shape(q_values)[0])[:, None]\n",
    "        selected_q_values = tf.gather_nd(q_values, tf.concat([batch_indices, actions], axis=-1))\n",
    "        values = tf.reduce_max(selected_q_values, axis=-1, keepdims=True)\n",
    "        advantages = selected_q_values - values\n",
    "\n",
    "        # Transform values and advantages using the global state\n",
    "        transformed_values = [self.value_transformers[i](global_state, values[:, i]) for i in range(self.num_agents)]\n",
    "        transformed_advantages = [self.advantage_transformers[i](global_state, advantages[:, i]) for i in range(self.num_agents)]\n",
    "\n",
    "        # Stack and aggregate transformed values and advantages\n",
    "        transformed_values = tf.stack(transformed_values, axis=1)\n",
    "        transformed_advantages = tf.stack(transformed_advantages, axis=1)\n",
    "\n",
    "        lambda_weights = self.lambda_weights(global_state)\n",
    "        lambda_weights = tf.expand_dims(lambda_weights, axis=-1)\n",
    "        joint_q_value = tf.reduce_sum(transformed_values + lambda_weights * transformed_advantages, axis=1)\n",
    "        return joint_q_value  # Shape: (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0a6e7-63c8-4552-9a2a-0efcdda8f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with epsilon-greedy policy and global state concatenation\n",
    "for episode in range(total_episodes):\n",
    "    # Reset environment and initialize rewards\n",
    "    states = env.reset()\n",
    "    episode_rewards = [0 for _ in range(num_agents)]\n",
    "\n",
    "    for t in range(steps_per_episode):\n",
    "        actions = []\n",
    "        for i in range(num_agents):\n",
    "            # Select action for each agent with epsilon-greedy policy\n",
    "            action = select_action(i, states[i], epsilon_values[i])\n",
    "            actions.append(action)\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_states, rewards = env.step(actions)\n",
    "\n",
    "        # Concatenate next states to form the next global state\n",
    "        global_state = np.concatenate([states[i] for i in range(num_agents)])\n",
    "\n",
    "        # Store experiences in the replay buffer\n",
    "        for i in range(num_agents):\n",
    "            buffers[i].add((states[i], actions[i], rewards[i], next_states[i]))\n",
    "\n",
    "        # Train the QPLEX model with the global state concatenation\n",
    "        if len(buffers[0]) >= batch_size:\n",
    "            batch_states = np.array([buffer.sample(batch_size)[0] for buffer in buffers])\n",
    "            batch_actions = np.array([buffer.sample(batch_size)[1] for buffer in buffers])\n",
    "            batch_rewards = np.array([buffer.sample(batch_size)[2] for buffer in buffers])\n",
    "            batch_next_states = np.array([buffer.sample(batch_size)[3] for buffer in buffers])\n",
    "\n",
    "            # Concatenate batch of states for global state input\n",
    "            batch_global_state = np.concatenate([batch_states[:, i] for i in range(num_agents)], axis=-1)\n",
    "\n",
    "            # Train step with QPLEX\n",
    "            loss = train_step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_global_state)\n",
    "\n",
    "        # Move to next state\n",
    "        states = next_states\n",
    "\n",
    "    # Decay epsilon and track rewards\n",
    "    for i in range(num_agents):\n",
    "        epsilon_values[i] = max(min_epsilon, epsilon_values[i] * epsilon_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
