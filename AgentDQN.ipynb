{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31ec4b58-3b6e-494b-919e-1f7d3c0ac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a1b88-be36-4eb8-bd79-faee4c32a96d",
   "metadata": {},
   "source": [
    "## Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b43b8e8c-4f96-4dd6-8601-3d57dcff5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEnvironment:\n",
    "    def __init__(self, environment, cost, num_states):\n",
    "        self.environment = environment\n",
    "        self.cost = cost\n",
    "        self.num_states = num_states\n",
    "        self.state = np.random.randint(0, self.num_states)\n",
    "\n",
    "    def reset(self):\n",
    "        # random action to cater for instances where some states were not visited by the agent\n",
    "        n_actions = len(self.environment.keys())\n",
    "        r_action = np.random.randint(0, n_actions)\n",
    "        \n",
    "        # obtain set of possible start states\n",
    "        trans_mat = self.environment[r_action].index.tolist()\n",
    "\n",
    "        # obtain the starting state\n",
    "        self.state = np.random.choice(trans_mat)\n",
    "        #self.state = np.random.randint(0, self.num_states)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # obtain the possible next states\n",
    "        ## \n",
    "        #print('action {}'.format(action))\n",
    "        #print('state {}'.format(state))\n",
    "        \n",
    "        pos_next_states = self.environment[action].columns.tolist()\n",
    "        p_j = self.environment[action].loc[state].values\n",
    "        next_state = np.random.choice(pos_next_states, p=p_j)\n",
    "        cost = self.cost[action][state]\n",
    "        done = False\n",
    "        return next_state, cost, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89f2e8-1e6f-402c-b8f4-e5408f23bed1",
   "metadata": {},
   "source": [
    "## Setting Up the Memory Externally for Future Import (Multiagent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695e10e7-2ec5-4e2a-805b-5d83b53d254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentMemory:\n",
    "    def __init__(self, max_length):\n",
    "        self.memory = deque(maxlen=max_length)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size, num_states):\n",
    "        batch_info = random.sample(self.memory, batch_size)\n",
    "        states, actions, next_states, costs, dones = zip(*batch_info)\n",
    "        #print(' memory next_states {}'.format(next_states)\n",
    "\n",
    "        # onehot encode states and next states\n",
    "        states = [int(s) for s in states]\n",
    "        next_states = [int(ns) for ns in next_states]\n",
    "\n",
    "        id_vector = np.eye(num_states)\n",
    "        states = id_vector[states]\n",
    "        next_states = id_vector[next_states]\n",
    "        #actions\n",
    "        return states, actions, next_states, costs, np.array(dones)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7e305-10b5-4b72-ac09-84f3097763c1",
   "metadata": {},
   "source": [
    "## Building the Q Network\n",
    "* Goal:\n",
    "    - Considering that the solution to the Bellman is unique. Result consistency and reproducibility is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f5add1-cc35-4a7e-9813-e3daa9bd3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDQN(tf.keras.Model):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim):\n",
    "        super(AgentDQN, self).__init__()\n",
    "        # Define network layers\n",
    "        self.h1 = tf.keras.layers.Dense(hidden_dim, activation='linear', kernel_initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.h2 = tf.keras.layers.Dense(16, activation='linear', kernel_initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.h3 = tf.keras.layers.Dense(8, activation = 'linear', kernel_initializer=tf.keras.initializers.GlorotNormal())\n",
    "        self.q_vals = tf.keras.layers.Dense(num_actions, activation='linear')\n",
    "\n",
    "    def call(self, state_onehot):\n",
    "        # forward pass\n",
    "        x = self.h1(state_onehot)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        q_values = self.q_vals(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d126e-9535-42ef-81d7-0f38c2390da9",
   "metadata": {},
   "source": [
    "## Compiling the DQN and Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92da8a3c-8311-4eb0-92d1-9d69cd4652b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNBuild:\n",
    "    def __init__(self, environment, cost, num_states, num_actions, hidden_dim, gamma = 0.95, lr=0.01, memory_length = 1000):\n",
    "        # initialize the environment\n",
    "        self.env = AgentEnvironment(environment=environment, cost=cost,num_states=num_states)\n",
    "        # initialize the AgentDQN\n",
    "        self.q_model = AgentDQN(num_states = num_states, num_actions = num_actions, hidden_dim=hidden_dim)\n",
    "        # initalize the target network\n",
    "        self.target_q_model = AgentDQN(num_states = num_states, num_actions = num_actions, hidden_dim=hidden_dim)\n",
    "        # intialize the agent memory\n",
    "        self.buffer = AgentMemory(max_length = memory_length)\n",
    "        # discount rate\n",
    "        self.gamma = gamma\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.train_step_counter = 0\n",
    "        #self.target_update_period = target_update_period\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # update target network at intervals to match the weights of the q-network\n",
    "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        # choose action based on epsolon-greedy strategy with (np.argmin) for cost minimization\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            # onehot encode the state\n",
    "            state_onehot = tf.expand_dims(tf.one_hot(state, self.num_states), axis=0)\n",
    "            # predict q_values\n",
    "            q_values = self.target_q_model(state_onehot)\n",
    "\n",
    "            # select action with minimum q_value\n",
    "            return tf.argmin(q_values, axis=1).numpy()[0] \n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, costs, next_states, dones, batch_size, use_target_network, target_update_period):\n",
    "        # obtain the loss function\n",
    "        mse_ = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Calculate current Q-values\n",
    "            current_q_values = self.q_model(states)\n",
    "            #print(f'current q shape{current_q_values.shape}')\n",
    "            \n",
    "            # Get target Q-values base\n",
    "            target_q_values = self.target_q_model(states).numpy() if use_target_network else  self.q_model(states).numpy()\n",
    "            backup_q = self.target_q_model(next_states).numpy() if use_target_network else  self.q_model(next_states).numpy() # copy current q values\n",
    "            \n",
    "            # Calculate backup Q-values for next states\n",
    "            #backup_q = self.q_model(next_states).numpy()\n",
    "            #print(f'backup_q shape {backup_q.shape}')\n",
    "\n",
    "            # Calculate target Q-values with Bellman equation\n",
    "            target_q = costs + self.gamma * np.min(backup_q, axis=1) * (1-dones)\n",
    "            target_q_values[np.arange(batch_size), actions]= target_q\n",
    "            target_q_values = tf.convert_to_tensor(target_q_values)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = mse_(target_q_values, current_q_values)\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.q_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_model.trainable_variables))\n",
    "\n",
    "        # update the target network counted\n",
    "        self.train_step_counter += 1\n",
    "        if use_target_network and target_update_period and self.train_step_counter % target_update_period == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, episodes, steps_per_episode, training_cost=1e5, batch_size=32, epsilon=1, epsilon_min=0.01, epsilon_decay=0.9995, episodic=False, terminal_period = False,\n",
    "             use_target_network = False, target_update_period = None):\n",
    "        # initialize old policy to be used for convergence check\n",
    "        previous_policy = np.zeros(shape=(1, self.num_states))\n",
    "        state = self.env.reset() # incase episodic is False\n",
    "\n",
    "        training_cost = training_cost\n",
    "        total_cost = 0\n",
    "        \n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            terminal_period_counter = 0\n",
    "            \n",
    "            for step_ in range(steps_per_episode):\n",
    "                terminal_period_counter += 1\n",
    "                # choose action\n",
    "                action = self.choose_action(state=state, epsilon=epsilon)\n",
    "\n",
    "                #obtain the one step transition information\n",
    "                next_state, cost, done = self.env.step(state, action)\n",
    "\n",
    "                # update the dones if episodic\n",
    "                if terminal_period and terminal_period_counter % terminal_period == 0:\n",
    "                    done = True\n",
    "                if terminal_period == False and episode == episodes-1 and step_ == steps_per_episode:\n",
    "                    done = True\n",
    "\n",
    "                # store experience in memory\n",
    "                self.buffer.add((state, action, next_state, cost, done))\n",
    "\n",
    "                if len(self.buffer)>= batch_size:\n",
    "                    # unpack the batch_info\n",
    "                    states, actions, next_states, costs, dones = self.buffer.sample(batch_size, num_states=self.num_states)\n",
    "\n",
    "                    # train the model\n",
    "                    loss = self.train_step(states = states, actions = actions, \n",
    "                                           next_states = next_states, costs = costs, \n",
    "                                           dones = dones, batch_size=batch_size, \n",
    "                                           use_target_network = use_target_network,\n",
    "                                          target_update_period = target_update_period)\n",
    "\n",
    "\n",
    "                # update state and total cost\n",
    "                state = next_state\n",
    "                training_cost -= cost\n",
    "                total_cost += cost\n",
    "                # epsilon decay\n",
    "                epsilon = max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "            # check for convergence\n",
    "            # obtain current policy\n",
    "            current_policy = self.get_policy()\n",
    "            if np.array_equal(current_policy, previous_policy):\n",
    "                print(f'Convergence Reached With Statble Policy at {episode}')\n",
    "                print(f'Optimal Policy: {current_policy}')\n",
    "                break\n",
    "            else:\n",
    "                previous_policy = current_policy\n",
    "                \n",
    "            # reset the starting state if it is the case the the system resets every year\n",
    "            if episodic:\n",
    "                state = self.env.reset()\n",
    "            \n",
    "\n",
    "    def get_policy(self):\n",
    "        # check if the current policy is equal to the new policy\n",
    "        id_vector = np.eye(self.num_states)\n",
    "        q_values = self.q_model(id_vector)\n",
    "        return np.argmin(q_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9ef98-4b86-4835-ad55-ad7a6ea897e9",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a46258-943d-463d-b218-15ffa38879b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data from the file\n",
    "with open('transition_matrices/transition_matrices_3_agents.json', 'r') as file:\n",
    "    json_string = file.read()      # Read JSON string from file\n",
    "    agent_information = json.loads(json_string) # Parse the JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29fd15-660a-4ff5-93c4-8138a5878b26",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae44a02-782c-4f6f-bceb-b20787a12c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix, costs = agent_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "609c5bab-40f8-4a07-a375-fb62615e73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = len(costs)\n",
    "num_actions = 2\n",
    "\n",
    "# convert all\n",
    "for agent in range(num_agents):\n",
    "    transition_matrix[agent] = {int(k):v for k, v in transition_matrix[agent].items()}\n",
    "    costs[agent] = {int(k) : v for k, v in costs[agent].items()}\n",
    "\n",
    "    for action in range(num_actions):\n",
    "        transition_matrix[agent][action] = pd.DataFrame(json.loads(transition_matrix[agent][action]))\n",
    "        # obtain the columns\n",
    "        agent_action_cols = [float(i) for i in transition_matrix[agent][action].columns.tolist()]\n",
    "        transition_matrix[agent][action].columns = agent_action_cols\n",
    "        costs[agent][action] = {int(k) : v for k, v in costs[agent][action].items()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adbeb081-d23f-4d09-bfd5-61a41bebf1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0  1.0  2.0\n",
       "0  0.0  0.5  0.5\n",
       "1  1.0  0.0  0.0\n",
       "2  0.0  1.0  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bdc0b27-692d-4a59-a8c9-a1ba70ecec50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.        , 0.33333333])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix[0][0].loc[2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d645f-9e8e-4bef-bde6-040b111e4143",
   "metadata": {},
   "source": [
    "## obtain sample agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d06089-586a-4775-89dd-1f45f9e3a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix_ = transition_matrix[0]\n",
    "cost = costs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7aa99a1-b9b4-497c-935a-0c10042ba2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 0.0, 1: 0, 2: 160.73, 3: 118.19},\n",
       " 1: {0: 0.0, 1: 28.67, 2: 60.73, 3: 18.19}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b709a275-67c4-49cb-b16f-478b068e6f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.0       1.0       2.0\n",
       "0  0.000000  0.333333  0.666667\n",
       "1  0.000000  0.000000  1.000000\n",
       "2  0.666667  0.000000  0.333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1287ad08-1a4a-416a-910b-2145a9cc465d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0  1.0  2.0\n",
       "0  0.0  0.0  1.0\n",
       "1  1.0  0.0  0.0\n",
       "2  0.2  0.4  0.4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee7b5f-27fa-4f8a-bca3-1145f6298f42",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19c03219-4c2a-4b74-8a8a-8e6f1c655c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dqn = DQNBuild(environment = transition_matrix_, cost = cost, num_states = 3, num_actions = 2,\n",
    "                    hidden_dim = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d8ce59-dcdf-4398-8724-f229ed736827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence Reached With Statble Policy at 2\n",
      "Optimal Policy: [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "agent_dqn.train(episodes = 5, steps_per_episode = 365, batch_size = 32, terminal_period = False, episodic=False, use_target_network=True, target_update_period = 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2029f-a1c4-4cf8-97e5-ce453c6ec24d",
   "metadata": {},
   "source": [
    "# Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "718ec8e7-85ed-4bc1-a05c-9cbe8b1ee4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, cost, n_actions, tol, n_state):\n",
    "    # Initialize the value function (V) and policy arrays\n",
    "    v_new = np.zeros(shape=(len(n_state), 1))\n",
    "\n",
    "    # initialize the starting policy\n",
    "    policy_o = np.zeros(shape=v_new.shape)\n",
    "    # initialize array to store new policy\n",
    "    policy_n = policy_o.copy()\n",
    "    \n",
    "    while True:\n",
    "        v_n = v_new.copy()\n",
    "        \n",
    "        for state in n_state:\n",
    "            action_value = []\n",
    "            for action in range(n_actions):\n",
    "                # Obtain the cost of the action for the current state\n",
    "                c_i = cost[action][state]\n",
    "                # Obtain the transition probabilities for the current state and action\n",
    "                p_ij = environment[action].loc[state].values\n",
    "                # Calculate the value for the state-action pair\n",
    "                v_ = (c_i + np.dot(p_ij, v_n)).item()\n",
    "                # Append the action value\n",
    "                action_value.append(v_)\n",
    "\n",
    "            # Update v_new for the current state with the minimum action value\n",
    "            v_new[state] = np.min(action_value)\n",
    "            # Update the policy with the action that yields the minimum value\n",
    "            policy_n[state] = np.argmin(action_value)\n",
    "\n",
    "           # print('v_new and v_check:{}'.format(v_new - v_n))\n",
    "\n",
    "        # Calculate convergence bounds\n",
    "        m_n = np.min(v_new - v_n)  # Lower bound\n",
    "        M_n = np.max(v_new - v_n)  # Upper bound\n",
    "\n",
    "        # Convergence check\n",
    "        if (np.max(np.abs(v_new - v_n)) <= tol) or np.array_equal(policy_n, policy_o):\n",
    "            break\n",
    "\n",
    "        # Copy the current values to v_n for this iteration\n",
    "        policy_o = policy_n\n",
    "    return policy_n.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04770906-656a-4923-b0df-7883ed11cf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(environment = transition_matrix_, cost = cost, n_actions=2, tol = 0.0001, n_state=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6510d0-fa9c-4be2-9505-1b331ed6ae33",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- DQN model obtains the solution as the Value iteration algorithm. Model works well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
