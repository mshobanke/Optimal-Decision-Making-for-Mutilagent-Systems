{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee0d4b5-e9c7-40b3-99b0-d764922e0e2f",
   "metadata": {
    "id": "6ee0d4b5-e9c7-40b3-99b0-d764922e0e2f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f669e92d-77b2-4893-914d-33b39a438fd7",
   "metadata": {
    "id": "f669e92d-77b2-4893-914d-33b39a438fd7"
   },
   "source": [
    "## Single Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b88d77-5575-4c83-9d13-68586d0f968a",
   "metadata": {
    "id": "d2b88d77-5575-4c83-9d13-68586d0f968a"
   },
   "outputs": [],
   "source": [
    "class AgentEnvironment:\n",
    "    def __init__(self, environment, cost, num_states):\n",
    "        self.environment = environment\n",
    "        self.cost = cost\n",
    "        self.num_states = num_states\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(0, self.num_states)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # obtain the possible next states\n",
    "        pos_next_states = self.environment[action].columns.tolist()\n",
    "\n",
    "        # obtain the transition probability of the corr. action and current state\n",
    "        p_j = self.environment[action].loc[state].values\n",
    "\n",
    "        # randomly select the next state based on the probability vector obtained\n",
    "        next_state = np.random.choice(pos_next_states, p=p_j)\n",
    "\n",
    "        # obtain the corresponding \n",
    "        cost = self.cost[action][state]\n",
    "\n",
    "        # done flag to signal terminal state\n",
    "        done = False\n",
    "\n",
    "        return next_state, cost, done\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "703a2e44-0b53-4db3-8db9-41350539af78",
   "metadata": {
    "id": "703a2e44-0b53-4db3-8db9-41350539af78"
   },
   "source": [
    "## Single Agent Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b934ecc9-13f1-44d8-8869-9b0b25ed2892",
   "metadata": {
    "id": "b934ecc9-13f1-44d8-8869-9b0b25ed2892"
   },
   "outputs": [],
   "source": [
    "class AgentMemory:\n",
    "    def __init__(self, max_length):\n",
    "        self.memory = deque(maxlen=max_length)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size, num_states):\n",
    "        batch_info = random.sample(self.memory, batch_size)\n",
    "        states, actions, next_states, costs, dones = zip(*batch_info)\n",
    "\n",
    "        # onehot encode states and next states\n",
    "        states = [int(s) for s in states]\n",
    "        next_states = [int(ns) for ns in next_states]\n",
    "\n",
    "        id_vector = np.eye(num_states)\n",
    "        states = id_vector[states]\n",
    "        next_states = id_vector[next_states]\n",
    "\n",
    "        return states, np.array(actions).reshape(-1, 1), next_states, costs, np.array(dones) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "695e7502-d66f-4be2-a6b7-1f21ef0b1de3",
   "metadata": {
    "id": "695e7502-d66f-4be2-a6b7-1f21ef0b1de3"
   },
   "source": [
    "## Single Agent DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2sRDECIgRgGt",
   "metadata": {
    "id": "2sRDECIgRgGt"
   },
   "outputs": [],
   "source": [
    "class AgentDQN(tf.keras.Model):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim):\n",
    "        super(AgentDQN, self).__init__()\n",
    "        # Define network layers\n",
    "        self.h1 = tf.keras.layers.Dense(hidden_dim, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h2 = tf.keras.layers.Dense(16, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h3 = tf.keras.layers.Dense(8, activation = 'linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.q_vals = tf.keras.layers.Dense(num_actions, activation='linear')\n",
    "\n",
    "    def call(self, state_onehot):\n",
    "        # forward pass\n",
    "        x = self.h1(state_onehot)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        q_values = self.q_vals(x)\n",
    "        return q_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8248837-c9ee-4a74-bd4c-d8557e6376cf",
   "metadata": {
    "id": "d8248837-c9ee-4a74-bd4c-d8557e6376cf"
   },
   "source": [
    "## Multi Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db344e66-632b-497a-b860-f3581331ce4f",
   "metadata": {
    "id": "db344e66-632b-497a-b860-f3581331ce4f"
   },
   "outputs": [],
   "source": [
    "class MultiAgentEnvironment:\n",
    "    def __init__(self, num_agents, transition_matrices, costs, num_states):\n",
    "        self.num_agents = num_agents\n",
    "        # initialize the environment for each agent\n",
    "        self.agents = [AgentEnvironment(environment=transition_matrices[agent], cost=cost[agent], num_states=num_states) for agent in range(self.num_agents)]\n",
    "\n",
    "    def reset(self):\n",
    "        # obtain list of random start states for each agent\n",
    "        return [agent.reset() for agent in self.agents] # list of start states for each agent shape num_agents by states\n",
    "\n",
    "    def step(self, states, actions):\n",
    "        next_states = [] # list of start states for each agent shape: num_agents by next_states (num_agents, 1)\n",
    "        costs = []   # list of start states for each agent shape: num_agents by costs (num_agents, 1)\n",
    "        dones = []  # list of start states for each agent shape: num_agents by dones  (num_agents, 1)\n",
    "\n",
    "        for i, (state, action) in enumerate(zip(states, actions)): # states, actions, is a list of states and actions of all agents\n",
    "            next_state, cost, done = self.agents[i].step(state, action)\n",
    "            # append next states, costs, dones\n",
    "            next_states.append(next_state)\n",
    "            costs.append(cost)\n",
    "            dones.append(done)\n",
    "\n",
    "        return next_states, costs, dones\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c937997-f860-464e-9a9f-cb2769449760",
   "metadata": {},
   "source": [
    "## Transformation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "WE_EXk9dRYm5",
   "metadata": {
    "id": "WE_EXk9dRYm5"
   },
   "outputs": [],
   "source": [
    "class AgentTransformation(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(AgentTransformation, self).__init__()\n",
    "        self.h1 = tf.keras.layers.Dense(hidden_dim, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h2 = tf.keras.layers.Dense(16, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h3 = tf.keras.layers.Dense(8, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.weight_ = tf.keras.layers.Dense(1, activation='softplus') # positive weights to ensure monotonicity\n",
    "        self.bias_ = tf.keras.layers.Dense(1, activation='softplus') # positive biases to ensure monotonicity\n",
    "\n",
    "    def call(self, global_state, agent_values):\n",
    "        x = self.h1(global_state)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        weight = self.weight_(x)\n",
    "        bias = self.bias_(x)\n",
    "        transformation_value = (weight*agent_values) + bias\n",
    "\n",
    "        return transformation_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85wVCMXAOjPW",
   "metadata": {
    "id": "85wVCMXAOjPW"
   },
   "source": [
    "# Mixing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9wPFjTlt5jue",
   "metadata": {
    "id": "9wPFjTlt5jue"
   },
   "outputs": [],
   "source": [
    "class MixingNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_states, num_actions, num_agents, hidden_dim=64):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.num_agents = num_agents\n",
    "        # initialize the DQN for each agent to be used to obtain the Q values\n",
    "        self.agents_dqns = [AgentDQN(num_states = num_states, num_actions=num_actions, hidden_dim = hidden_dim) for agent in range(num_agents)]\n",
    "\n",
    "        # intialize the transformation network to be used to calculate the transformed a's and v's for each agent\n",
    "        self.agents_v_transforms = [AgentTransformation(hidden_dim = hidden_dim) for agent in range(num_agents)]\n",
    "        self.agents_a_transforms = [AgentTransformation(hidden_dim = hidden_dim) for agent in range(num_agents)]\n",
    "\n",
    "        # create feedforward network to obtain positive lambda weights to ensure monotonicity \n",
    "        self.lambda_weights = tf.keras.layers.Dense(num_agents, activation='softplus')\n",
    "\n",
    "    def call(self, states, actions, global_states, batch_size, target_network=False):\n",
    "        # at this point each states and actions will have shape num_agents, batch_size, 1\n",
    "\n",
    "        # calculate the q values for each agent\n",
    "        q_values = [self.agents_dqns[agent](states[agent]) for agent in range(self.num_agents)] # shape num_agents, batch_size, 2\n",
    "        # there might be an issue here\n",
    "\n",
    "        # stack the q_values on the batch_size dimension to make the shape batch_size, num_agents, q_values\n",
    "        q_values = tf.stack(q_values, axis=1)\n",
    "\n",
    "        if target_network:\n",
    "          # obtain the q-values based on the e-greedy actions\n",
    "          q_action_chosen = tf.reduce_min(q_values, axis=-1) # shape(batch_size, num_agents)\n",
    "\n",
    "        else:\n",
    "          #stack the actions as well\n",
    "          actions_stacked = tf.stack(actions, axis=1)\n",
    "\n",
    "          # # create indices that will be used to extract the q-values of chosen actions for each agent\n",
    "          batch_indices = tf.reshape(tf.range(batch_size), (-1,1)) # batch_size by 1\n",
    "          agent_indices = tf.range(self.num_agents) # shape (num_agents, [1])\n",
    "\n",
    "          # create the batch array to be used for extraction\n",
    "          batch_array_indices = tf.stack(tf.meshgrid(batch_indices[:, 0], agent_indices, indexing='ij'), axis=-1) # batch_size, num_agents, 2 [i,j values]\n",
    "\n",
    "          # add the action to the batch_array indices in order to get the corresponding q_values\n",
    "          actions_stacked = tf.cast(actions_stacked, dtype=tf.int32)\n",
    "          batch_array_indices = tf.concat([batch_array_indices, actions_stacked], axis=-1)\n",
    "\n",
    "          # obtain the q-values based on the e-greedy actions\n",
    "          q_action_chosen = tf.gather_nd(q_values, batch_array_indices) # shape (batch_size, num_agents)\n",
    "\n",
    "        # calculate the v which is taken as the minimum q value for each agent in each batch\n",
    "        v_value = tf.reduce_min(q_values, axis=-1) # shape(batch_size, num_agents)\n",
    "\n",
    "        # calculate the advantage\n",
    "        agent_advantages = tf.expand_dims(q_action_chosen - v_value, axis=-1)\n",
    "        v_value = tf.expand_dims(v_value, axis=-1)\n",
    "\n",
    "        # Calculate the Transformed Advantage\n",
    "        transformed_advantages = [\n",
    "            self.agents_a_transforms[agent](global_state=global_states, agent_values=agent_advantages[:, agent]) for agent in range(self.num_agents)\n",
    "        ] # num_agents, batch_size, 1\n",
    "        # Calculate the transformed V - Value\n",
    "        transformed_v_value = [\n",
    "            self.agents_v_transforms[agent](global_state=global_states, agent_values=v_value[:, agent]) for agent in range(self.num_agents)\n",
    "        ] # shape: num_agents, batch_size, 1\n",
    "\n",
    "        # obtain the lambda weights\n",
    "        transformed_adv = tf.stack(transformed_advantages, axis=1) # shape becomes batch_size, num_agents, 1\n",
    "        transformed_values = tf.stack(transformed_v_value, axis=1) # shape = batch_size, num_agents, 1\n",
    "\n",
    "        # calculate the lambda weight using the global state\n",
    "        lambda_w = self.lambda_weights(global_states) # outputs a lambda for each agent\n",
    "        lambda_w = tf.expand_dims(lambda_w, axis=-1)\n",
    "\n",
    "        # calculate the joint q value\n",
    "        joint_q_value = tf.reduce_sum(\n",
    "            transformed_values + (lambda_w*transformed_adv), axis=1\n",
    "        )\n",
    "\n",
    "        return joint_q_value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcbb1e4c-b755-4db7-b0c4-de90cb79c3b6",
   "metadata": {},
   "source": [
    "## Compiling the QPLEX and Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "vZCEdiSs-rWX",
   "metadata": {
    "id": "vZCEdiSs-rWX"
   },
   "outputs": [],
   "source": [
    "class QPLEXBuild:\n",
    "    def __init__(self, num_agents, num_states, num_actions, transition_matrices, costs, lr=0.01, memory_length=1000, hidden_dim=64, gamma=0.95):\n",
    "        # initialize the parameters\n",
    "        self.env = MultiAgentEnvironment(num_agents=num_agents, transition_matrices=transition_matrices, costs=costs, num_states=num_states)\n",
    "        self.buffers = [AgentMemory(max_length = memory_length) for agent in range(num_agents)]\n",
    "        self.qplex_model = MixingNetwork(num_states=num_states, num_actions=num_actions, num_agents=num_agents, hidden_dim=hidden_dim)\n",
    "        self.target_qplex_model = MixingNetwork(num_states=num_states, num_actions=num_actions, num_agents=num_agents, hidden_dim=hidden_dim)\n",
    "\n",
    "        # initialize training parameters\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.num_agents = num_agents\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # train_step_counter for updating target network\n",
    "        self.train_step_counter = 0\n",
    "\n",
    "    def choose_action(self, agent_index, state, epsilon):\n",
    "        # choose action based on epsilon-greedy probability\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            # onehot the received agent state\n",
    "            state_onehot = tf.expand_dims(tf.one_hot(state, self.num_states), axis=0)\n",
    "            # predict q_values\n",
    "            q_values = self.target_qplex_model.agents_dqns[agent_index](state_onehot)\n",
    "\n",
    "            # select action with minimum q_value\n",
    "            return tf.argmin(q_values, axis=1).numpy()[0]  # note that this is for a single agent\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_qplex_model.set_weights(self.qplex_model.get_weights())\n",
    "\n",
    "    def train_step(self, states, actions, costs, next_states, global_states, global_next_states, batch_size, target_update_period, done = False):\n",
    "        # initialize the loss\n",
    "        mse_ = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # obtaint the current q values\n",
    "            current_joint_q = self.qplex_model(states=states, actions=actions, global_states=global_states, batch_size=batch_size) # batch_size, 1\n",
    "\n",
    "            # calculate the backup q\n",
    "            backup_joint_q = self.target_qplex_model(states=next_states, actions=actions, global_states=global_next_states,\n",
    "                                          batch_size=batch_size, target_network=True).numpy() # batch_size, 1\n",
    "\n",
    "            # calculate the target q with bellman equation\n",
    "            target_joint_q = tf.convert_to_tensor(costs + self.gamma * backup_joint_q * (1-done))\n",
    "\n",
    "            loss = mse_(target_joint_q, current_joint_q)\n",
    "\n",
    "        # compute the gradient\n",
    "        grads = tape.gradient(loss, self.qplex_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.qplex_model.trainable_variables))\n",
    "\n",
    "        # update the target network counted\n",
    "        self.train_step_counter += 1\n",
    "        if self.train_step_counter % target_update_period == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, episodes, steps_per_episode, training_cost=1e5, batch_size=32, epsilon=1, epsilon_min=0.01, epsilon_decay=0.9995, episodic=False, terminal_period = False,\n",
    "             target_update_period = None):\n",
    "        # initialize epsilon for all agents\n",
    "        epsilon_per_agent = [epsilon for agent in range(self.num_agents)]\n",
    "        # initialize old policy - do this later\n",
    "        previous_policy_per_agent = np.random.rand(self.num_agents, self.num_states)\n",
    "\n",
    "        training_cost = [training_cost for agent in range(self.num_agents)]\n",
    "        total_cost = [0 for agent in range(self.num_agents)]\n",
    "\n",
    "        # initialize state in case it is not episodic\n",
    "        states = self.env.reset()\n",
    "        losses = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            terminal_period_counter = 0\n",
    "\n",
    "            for step_ in range(steps_per_episode):\n",
    "                terminal_period_counter += 1\n",
    "\n",
    "                # each agent chooses an action\n",
    "                actions = [self.choose_action(agent_index=agent, state=states[agent], epsilon=epsilon_per_agent[agent]) for agent in range(self.num_agents)] # shape: num_agents, 1\n",
    "\n",
    "                # obtain the one step transition information\n",
    "                next_states, costs, _ = self.env.step(states=states, actions=actions) # no need for the dones per agent. shape for each one (num_agents, 1)\n",
    "\n",
    "                # store in memory\n",
    "                for agent_ in range(self.num_agents):\n",
    "                    self.buffers[agent_].append((states[agent_], actions[agent_], next_states[agent_], costs[agent_], _[agent_]))\n",
    "\n",
    "                # train if enough samples\n",
    "                if len(self.buffers[0]) >= batch_size:\n",
    "                    # sample experiences\n",
    "                    batch_info = [buffer.sample(batch_size, num_states=self.num_states) for buffer in self.buffers] # shape (num_agents, batch_size)\n",
    "\n",
    "                    # obtain batch information\n",
    "                    batch_states = np.array([data[0] for data in batch_info]) # num_agents, batch_size, [1]\n",
    "                    batch_actions = np.array([data[1] for data in batch_info])\n",
    "                    batch_next_states = np.array([data[2] for data in batch_info]) # num_agents, batch_size, [1]\n",
    "                    batch_costs_ = [data[3] for data in batch_info]\n",
    "\n",
    "                    # calculate the joint cost\n",
    "                    joint_bacth_costs = tf.expand_dims(tf.reduce_sum(tf.stack(batch_costs_, axis=1), axis=1), axis=-1)\n",
    "\n",
    "                    # create the global_current_state\n",
    "                    batch_global_state = tf.stack(batch_states.tolist(), axis=1) # batch_size, num_agents\n",
    "                    batch_global_next_state = tf.stack(batch_next_states.tolist(), axis=1) # batch_size, num_agents\n",
    "\n",
    "                    # reshape the global and global next states so that its batch size, num_states*2\n",
    "                    batch_global_state = tf.reshape(batch_global_state, (batch_global_state.shape[0], -1))\n",
    "                    batch_global_next_state = tf.reshape(batch_global_next_state, (batch_global_next_state.shape[0], -1))\n",
    "\n",
    "                    # update the dones if episodic\n",
    "                    done = False\n",
    "                    if terminal_period and terminal_period_counter % terminal_period == 0:\n",
    "                        done = True\n",
    "                    if terminal_period == False and episode == episodes-1 and step_ == steps_per_episode:\n",
    "                        done = True\n",
    "\n",
    "                    # calculate the loss\n",
    "                    loss = self.train_step(states =batch_states, actions=batch_actions, next_states=batch_next_states, costs=joint_bacth_costs,\n",
    "                                          global_states = batch_global_state, global_next_states=batch_global_next_state, batch_size=batch_size,\n",
    "                                          target_update_period=target_update_period, done=done)\n",
    "                    losses.append(loss)\n",
    "                    #print(f'Running: {loss}')\n",
    "\n",
    "                # update states\n",
    "                states = next_states\n",
    "                # update training cost, total_cost, epsilon\n",
    "                for ag_ in range(self.num_agents):\n",
    "                    training_cost[ag_] -= costs[ag_]\n",
    "                    total_cost[ag_] += costs[ag_]\n",
    "                    epsilon_per_agent[ag_] = max(epsilon_min, epsilon_per_agent[ag_]*epsilon_decay)\n",
    "\n",
    "            # check for convergence\n",
    "            # obtain current policy\n",
    "            current_policy = self.get_policy()\n",
    "            #print('Current Policy for All Agents \\n{}\\n'.format(current_policy))\n",
    "            if np.all(current_policy == previous_policy_per_agent):\n",
    "                print(f'Convergence Reached With Statble Policy at {episode}')\n",
    "                print(f'Optimal Policy: {current_policy}')\n",
    "                break\n",
    "            else:\n",
    "                previous_policy_per_agent = current_policy\n",
    "\n",
    "            # reset the starting state if it is the case the the system resets every year\n",
    "            if episodic:\n",
    "                state = self.env.reset()\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "        # get the policy for each agent\n",
    "        id_vector = np.eye(self.num_states)\n",
    "        q_values_agents = []\n",
    "\n",
    "        for agent in range(self.num_agents):\n",
    "            q_vals = self.qplex_model.agents_dqns[agent](id_vector)\n",
    "            q_vals = np.argmin(q_vals, axis=1)\n",
    "            q_values_agents.append(q_vals)\n",
    "\n",
    "        return np.array(q_values_agents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "tf0KWpZS-sWX",
   "metadata": {
    "id": "tf0KWpZS-sWX"
   },
   "source": [
    "## Testing the QPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hsLn9Xgi-maS",
   "metadata": {
    "id": "hsLn9Xgi-maS"
   },
   "outputs": [],
   "source": [
    "def unpack_data(transition_matrix, cost, num_actions):\n",
    "    # convert all\n",
    "    for agent in range(len(transition_matrix)):\n",
    "        transition_matrix[agent] = {int(k):v for k, v in transition_matrix[agent].items()}\n",
    "        cost[agent] = {int(k) : v for k, v in cost[agent].items()}\n",
    "\n",
    "        for action in range(num_actions):\n",
    "            transition_matrix[agent][action] = pd.DataFrame(json.loads(transition_matrix[agent][action]))\n",
    "            # obtain the columns\n",
    "            agent_action_cols = [float(i) for i in transition_matrix[agent][action].columns.tolist()]\n",
    "            transition_matrix[agent][action].columns = agent_action_cols\n",
    "            cost[agent][action] = {int(k) : v for k, v in cost[agent][action].items()}\n",
    "\n",
    "    return transition_matrix, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "uA2gIhLC-1_I",
   "metadata": {
    "id": "uA2gIhLC-1_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0\n",
      "Sample agent cost info: \n",
      "{0: {0: 0.0, 1: 0, 2: 160.73, 3: 118.19}, 1: {0: 0.0, 1: 28.67, 2: 60.73, 3: 18.19}}\n",
      "\n",
      "Sample agent transition matrix for Action = 0: \n",
      "{0:         0.0       1.0       2.0\n",
      "0  0.000000  0.333333  0.666667\n",
      "1  0.000000  0.000000  1.000000\n",
      "2  0.666667  0.000000  0.333333, 1:    0.0  1.0  2.0\n",
      "0  0.0  0.0  1.0\n",
      "1  1.0  0.0  0.0\n",
      "2  0.2  0.4  0.4}\n",
      "\n",
      "Sample agent transition Matrix for Action = 1: \n",
      "{0:    0.0   1.0   2.0\n",
      "0  0.0  0.25  0.75\n",
      "1  1.0  0.00  0.00\n",
      "2  0.4  0.20  0.40, 1:    0.0  1.0  2.0\n",
      "0  0.0  0.5  0.5\n",
      "1  1.0  0.0  0.0\n",
      "2  0.0  1.0  0.0}\n",
      "\n",
      "Agent 1\n",
      "Sample agent cost info: \n",
      "{0: {0: 0.0, 1: 0, 2: 160.73, 3: 118.19}, 1: {0: 0.0, 1: 28.67, 2: 60.73, 3: 18.19}}\n",
      "\n",
      "Sample agent transition matrix for Action = 0: \n",
      "{0:         0.0       1.0       2.0\n",
      "0  0.000000  0.333333  0.666667\n",
      "1  0.000000  0.000000  1.000000\n",
      "2  0.666667  0.000000  0.333333, 1:    0.0  1.0  2.0\n",
      "0  0.0  0.0  1.0\n",
      "1  1.0  0.0  0.0\n",
      "2  0.2  0.4  0.4}\n",
      "\n",
      "Sample agent transition Matrix for Action = 1: \n",
      "{0:    0.0   1.0   2.0\n",
      "0  0.0  0.25  0.75\n",
      "1  1.0  0.00  0.00\n",
      "2  0.4  0.20  0.40, 1:    0.0  1.0  2.0\n",
      "0  0.0  0.5  0.5\n",
      "1  1.0  0.0  0.0\n",
      "2  0.0  1.0  0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_ = 'data/dtmc_data.json'\n",
    "\n",
    "with open(file_, 'r') as file:\n",
    "  agents_information = json.load(file)\n",
    "\n",
    "\n",
    "# unpacking the data\n",
    "transition_matrices, cost = agents_information\n",
    "transition_matrices, costs = unpack_data(transition_matrix=transition_matrices, cost=cost, num_actions=2)\n",
    "\n",
    "# select sample agents\n",
    "idx_sample_agent = [0,1]\n",
    "trans_matrices_selected = [transition_matrices[i] for i in idx_sample_agent]\n",
    "costs_selected = [costs[i] for i in idx_sample_agent]\n",
    "\n",
    "for i in range(len(idx_sample_agent)):\n",
    "  print(f'Agent {i}')\n",
    "  print('Sample agent cost info: \\n{}\\n'.format(costs_selected[i]))\n",
    "  print('Sample agent transition matrix for Action = 0: \\n{}\\n'.format(trans_matrices_selected[0]))\n",
    "  print('Sample agent transition Matrix for Action = 1: \\n{}\\n'.format(trans_matrices_selected[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "R9i9ybpp_KAU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9i9ybpp_KAU",
    "outputId": "0686dfac-7e0c-489b-b8eb-de06c6d6b28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence Reached With Statble Policy at 2\n",
      "Optimal Policy: [[0 1 1]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# initialize the agent\n",
    "agents_qplex = QPLEXBuild(num_agents=2, num_states=3, num_actions=2, transition_matrices=trans_matrices_selected,\n",
    "                          costs=costs_selected)\n",
    "\n",
    "# train the model\n",
    "agents_qplex.train(episodes=5, steps_per_episode=365, epsilon_min=0.01, epsilon=1, target_update_period=10, epsilon_decay=0.95, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7a4fa12",
   "metadata": {},
   "source": [
    "## Applying Value Iteration Algorithm Decentrally to all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22a0adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, cost, n_actions, tol, n_state, gamma):\n",
    "    # Initialize the value function (V) and policy arrays\n",
    "    v_new = np.zeros(shape=(n_state, 1))\n",
    "\n",
    "    # initialize the starting policy\n",
    "    policy_o = np.zeros(shape=v_new.shape)\n",
    "    # initialize array to store new policy\n",
    "    policy_n = policy_o.copy()\n",
    "    \n",
    "    while True:\n",
    "        v_n = v_new.copy()\n",
    "        \n",
    "        for state in range(n_state):\n",
    "            action_value = []\n",
    "            for action in range(n_actions):\n",
    "                # Obtain the cost of the action for the current state\n",
    "                c_i = cost[action][state]\n",
    "                # Obtain the transition probabilities for the current state and action\n",
    "                p_ij = environment[action].loc[state].values\n",
    "                # Calculate the value for the state-action pair\n",
    "                v_ = (c_i + gamma * np.dot(p_ij, v_n)).item()\n",
    "                # Append the action value\n",
    "                action_value.append(v_)\n",
    "\n",
    "            # Update v_new for the current state with the minimum action value\n",
    "            v_new[state] = np.min(action_value)\n",
    "            # Update the policy with the action that yields the minimum value\n",
    "            policy_n[state] = np.argmin(action_value)\n",
    "\n",
    "           # print('v_new and v_check:{}'.format(v_new - v_n))\n",
    "\n",
    "        # Calculate convergence bounds\n",
    "        m_n = np.min(v_new - v_n)  # Lower bound\n",
    "        M_n = np.max(v_new - v_n)  # Upper bound\n",
    "\n",
    "        # Convergence check\n",
    "        if (np.max(np.abs(v_new - v_n)) <= tol) or np.array_equal(policy_n, policy_o):\n",
    "            break\n",
    "\n",
    "        # Copy the current values to v_n for this iteration\n",
    "        policy_o = policy_n\n",
    "    return policy_n.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f556658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for Agent 0: [0. 1. 1.]\n",
      "Policy for Agent 1: [1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Testing the VA algorithm on each agent separately\n",
    "\n",
    "for agent in idx_sample_agent:\n",
    "    policy = value_iteration(environment = trans_matrices_selected[agent], cost = costs_selected[agent], n_actions=2, tol = 0.0001, n_state=3, gamma=0.9)\n",
    "    print(f'Policy for Agent {agent}: {policy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07e2e48e",
   "metadata": {},
   "source": [
    "## Results\n",
    "Results show that the QPLEX CTDE approach performs as well as the DTDE implementation of Value Iteration. This indicates that all agents in the QPLEX are effectively learning\n",
    "\n",
    "\n",
    "* More experimentation and analysis can be found in the folder in the link below:\n",
    "https://github.com/mshobanke/Optimal-Decision-Making-for-Mutilagent-Systems/tree/main/implementation%20and%20learning%20dashboard\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
