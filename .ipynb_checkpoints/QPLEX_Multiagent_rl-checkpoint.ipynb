{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0R7jG9X2_AMD",
   "metadata": {
    "id": "0R7jG9X2_AMD"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "OanaEvEg_Ga_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "OanaEvEg_Ga_",
    "outputId": "b8e0db94-d4d5-4380-b807-fc0f18f0d6af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-77e33317-a732-4a75-a64a-13f4a26c8a0f\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-77e33317-a732-4a75-a64a-13f4a26c8a0f\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving multiagent_historical_information.json to multiagent_historical_information (1).json\n"
     ]
    }
   ],
   "source": [
    "data_upload = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee0d4b5-e9c7-40b3-99b0-d764922e0e2f",
   "metadata": {
    "id": "6ee0d4b5-e9c7-40b3-99b0-d764922e0e2f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669e92d-77b2-4893-914d-33b39a438fd7",
   "metadata": {
    "id": "f669e92d-77b2-4893-914d-33b39a438fd7"
   },
   "source": [
    "## Single Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2b88d77-5575-4c83-9d13-68586d0f968a",
   "metadata": {
    "id": "d2b88d77-5575-4c83-9d13-68586d0f968a"
   },
   "outputs": [],
   "source": [
    "class AgentEnvironment:\n",
    "    def __init__(self, environment, cost, num_states):\n",
    "        self.environment = environment\n",
    "        self.cost = cost\n",
    "        self.num_states = num_states\n",
    "        self.state = np.random.randint(0, self.num_states)\n",
    "\n",
    "    def reset(self):\n",
    "        # random action to cater for instances where some states were not visited by the agent\n",
    "        n_actions = len(self.environment.keys())\n",
    "        r_action = np.random.randint(0, n_actions)\n",
    "\n",
    "        # obtain set of possible start states\n",
    "        trans_mat = self.environment[r_action].index.tolist()\n",
    "\n",
    "        # obtain the starting state\n",
    "        self.state = np.random.choice(trans_mat)\n",
    "        #self.state = np.random.randint(0, self.num_states)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # obtain the possible next states\n",
    "        ##\n",
    "\n",
    "        pos_next_states = self.environment[action].columns.tolist()\n",
    "        p_j = self.environment[action].loc[state].values\n",
    "        next_state = np.random.choice(pos_next_states, p=p_j)\n",
    "        cost = self.cost[action][state]\n",
    "        done = False\n",
    "        return next_state, cost, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a2e44-0b53-4db3-8db9-41350539af78",
   "metadata": {
    "id": "703a2e44-0b53-4db3-8db9-41350539af78"
   },
   "source": [
    "## Single Agent Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b934ecc9-13f1-44d8-8869-9b0b25ed2892",
   "metadata": {
    "id": "b934ecc9-13f1-44d8-8869-9b0b25ed2892"
   },
   "outputs": [],
   "source": [
    "class AgentMemory:\n",
    "    def __init__(self, max_length):\n",
    "        self.memory = deque(maxlen=max_length)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def sample(self, batch_size, num_states):\n",
    "        batch_info = random.sample(self.memory, batch_size)\n",
    "        states, actions, next_states, costs, dones = zip(*batch_info)\n",
    "\n",
    "        # onehot encode states and next states\n",
    "        states = [int(s) for s in states]\n",
    "        next_states = [int(ns) for ns in next_states]\n",
    "\n",
    "        id_vector = np.eye(num_states)\n",
    "        states = id_vector[states]\n",
    "        next_states = id_vector[next_states]\n",
    "\n",
    "        return states, np.array(actions).reshape(-1, 1), next_states, costs, np.array(dones) #np.array(dones).reshape(-1,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e7502-d66f-4be2-a6b7-1f21ef0b1de3",
   "metadata": {
    "id": "695e7502-d66f-4be2-a6b7-1f21ef0b1de3"
   },
   "source": [
    "## Single Agent DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2sRDECIgRgGt",
   "metadata": {
    "id": "2sRDECIgRgGt"
   },
   "outputs": [],
   "source": [
    "class AgentDQN(tf.keras.Model):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim):\n",
    "        super(AgentDQN, self).__init__()\n",
    "        # Define network layers\n",
    "        self.h1 = tf.keras.layers.Dense(hidden_dim, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h2 = tf.keras.layers.Dense(16, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h3 = tf.keras.layers.Dense(8, activation = 'linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.q_vals = tf.keras.layers.Dense(num_actions, activation='linear')\n",
    "\n",
    "    def call(self, state_onehot):\n",
    "        # forward pass\n",
    "        x = self.h1(state_onehot)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        q_values = self.q_vals(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8248837-c9ee-4a74-bd4c-d8557e6376cf",
   "metadata": {
    "id": "d8248837-c9ee-4a74-bd4c-d8557e6376cf"
   },
   "source": [
    "## Multi Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db344e66-632b-497a-b860-f3581331ce4f",
   "metadata": {
    "id": "db344e66-632b-497a-b860-f3581331ce4f"
   },
   "outputs": [],
   "source": [
    "class MultiAgentEnvironment:\n",
    "    def __init__(self, num_agents, transition_matrices, costs, num_states):\n",
    "        self.num_agents = num_agents\n",
    "        # initialize the environment for each agent\n",
    "        self.agents = [AgentEnvironment(environment=transition_matrices[agent], cost=cost[agent], num_states=num_states) for agent in range(self.num_agents)]\n",
    "\n",
    "    def reset(self):\n",
    "        # obtain list of random start states for each agent\n",
    "        return [agent.reset() for agent in self.agents] # list of start states for each agent shape num_agents by states\n",
    "\n",
    "    def step(self, states, actions):\n",
    "        next_states = [] # list of start states for each agent shape: num_agents by next_states (num_agents, 1)\n",
    "        costs = []   # list of start states for each agent shape: num_agents by costs (num_agents, 1)\n",
    "        dones = []  # list of start states for each agent shape: num_agents by dones  (num_agents, 1)\n",
    "\n",
    "        for i, (state, action) in enumerate(zip(states, actions)): # states, actions, is a list of states and actions of all agents\n",
    "            next_state, cost, done = self.agents[i].step(state, action)\n",
    "            # append next states, costs, dones\n",
    "            next_states.append(next_state)\n",
    "            costs.append(cost)\n",
    "            dones.append(done)\n",
    "\n",
    "        return next_states, costs, dones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c937997-f860-464e-9a9f-cb2769449760",
   "metadata": {},
   "source": [
    "## Transformation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "WE_EXk9dRYm5",
   "metadata": {
    "id": "WE_EXk9dRYm5"
   },
   "outputs": [],
   "source": [
    "class AgentTransformation(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super(AgentTransformation, self).__init__()\n",
    "        self.h1 = tf.keras.layers.Dense(hidden_dim, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h2 = tf.keras.layers.Dense(16, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.h3 = tf.keras.layers.Dense(8, activation='linear', kernel_initializer= tf.keras.initializers.GlorotNormal())\n",
    "        self.weight_ = tf.keras.layers.Dense(1, activation='softplus')#, kernel_initializer='he_normal', name='v_weights')\n",
    "        self.bias_ = tf.keras.layers.Dense(1, activation='softplus')#, name='v-bias')\n",
    "\n",
    "    def call(self, global_state, agent_values):\n",
    "        x = self.h1(global_state)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        weight = self.weight_(x)\n",
    "        bias = self.bias_(x)\n",
    "        transformation_value = (weight*agent_values) + bias\n",
    "\n",
    "        return transformation_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85wVCMXAOjPW",
   "metadata": {
    "id": "85wVCMXAOjPW"
   },
   "source": [
    "# Mixing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9wPFjTlt5jue",
   "metadata": {
    "id": "9wPFjTlt5jue"
   },
   "outputs": [],
   "source": [
    "class MixingNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_states, num_actions, num_agents, hidden_dim=64):\n",
    "        super(MixingNetwork, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.num_agents = num_agents\n",
    "        # initialize the DQN for each agent to be used to obtain the Q values\n",
    "        self.agents_dqns = [AgentDQN(num_states = num_states, num_actions=num_actions, hidden_dim = hidden_dim) for agent in range(num_agents)]\n",
    "\n",
    "        # intialize the transformation network to be used to calculate the transformed a's and v's for each agent\n",
    "        self.agents_v_transforms = [AgentTransformation(hidden_dim = hidden_dim) for agent in range(num_agents)]\n",
    "        self.agents_a_transforms = [AgentTransformation(hidden_dim = hidden_dim) for agent in range(num_agents)]\n",
    "\n",
    "        # create feedforward network to obtain lambda for each agent\n",
    "        self.lambda_weights = tf.keras.layers.Dense(num_agents, activation='softplus')#), kernel_initializer='he_normal', name='lambda_weights')\n",
    "\n",
    "    def call(self, states, actions, global_states, batch_size, target_network=False):\n",
    "        # at this point each states and actions will have shape num_agents, batch_size, 1\n",
    "\n",
    "        # calculate the q values for each agent\n",
    "        q_values = [self.agents_dqns[agent](states[agent]) for agent in range(self.num_agents)] # shape num_agents, batch_size, 2\n",
    "        # there might be an issue here\n",
    "\n",
    "        # stack the q_values on the batch_size dimension to make the shape batch_size, num_agents, q_values\n",
    "        q_values = tf.stack(q_values, axis=1)\n",
    "\n",
    "        if target_network:\n",
    "          # obtain the q-values based on the e-greedy actions\n",
    "          q_action_chosen = tf.reduce_min(q_values, axis=-1) # shape(batch_size, num_agents)\n",
    "\n",
    "        else:\n",
    "          #stack the actions as well\n",
    "          actions_stacked = tf.stack(actions, axis=1)\n",
    "\n",
    "          # # create indices that will be used to extract the q-values for each agent\n",
    "          batch_indices = tf.reshape(tf.range(batch_size), (-1,1)) # batch_size by 1\n",
    "          agent_indices = tf.range(self.num_agents) # shape (num_agents, [1])\n",
    "\n",
    "          # create the batch array to be used for extraction\n",
    "          batch_array_indices = tf.stack(tf.meshgrid(batch_indices[:, 0], agent_indices, indexing='ij'), axis=-1) # batch_size, num_agents, 2 [i,j values]\n",
    "\n",
    "          # add the action to the batch_array indices in order to get the corresponding q_values\n",
    "          actions_stacked = tf.cast(actions_stacked, dtype=tf.int32)\n",
    "          batch_array_indices = tf.concat([batch_array_indices, actions_stacked], axis=-1)\n",
    "\n",
    "          # obtain the q-values based on the e-greedy actions\n",
    "          q_action_chosen = tf.gather_nd(q_values, batch_array_indices) # shape (batch_size, num_agents)\n",
    "\n",
    "        # calculate the v which is taken as the minimum q value for each agent in each batch\n",
    "        v_value = tf.reduce_min(q_values, axis=-1) # shape(batch_size, num_agents)\n",
    "\n",
    "        # calculate the advantage\n",
    "        agent_advantages = tf.expand_dims(q_action_chosen - v_value, axis=-1)\n",
    "        v_value = tf.expand_dims(v_value, axis=-1)\n",
    "\n",
    "        # Calculate the Transformed Advantage\n",
    "        transformed_advantages = [\n",
    "            self.agents_a_transforms[agent](global_state=global_states, agent_values=agent_advantages[:, agent]) for agent in range(self.num_agents)\n",
    "        ] # num_agents, batch_size, 1\n",
    "        # Calculate the transformed V - Value\n",
    "        transformed_v_value = [\n",
    "            self.agents_v_transforms[agent](global_state=global_states, agent_values=v_value[:, agent]) for agent in range(self.num_agents)\n",
    "        ] # shape: num_agents, batch_size, 1\n",
    "\n",
    "        # obtain the lambda weights\n",
    "        transformed_adv = tf.stack(transformed_advantages, axis=1) # shape becomes batch_size, num_agents, 1\n",
    "        transformed_values = tf.stack(transformed_v_value, axis=1) # shape = batch_size, num_agents, 1\n",
    "\n",
    "        # calculate the lambda weight using the global state\n",
    "        lambda_w = self.lambda_weights(global_states) # outputs a lambda for each agent\n",
    "        lambda_w = tf.expand_dims(lambda_w, axis=-1)\n",
    "\n",
    "        # calculate the joint q value\n",
    "        joint_q_value = tf.reduce_sum(\n",
    "            transformed_values + (lambda_w*transformed_adv), axis=1\n",
    "        )\n",
    "        #print([var.name for var in self.trainable_variables])\n",
    "\n",
    "        #joint_q_value = tf.reduce_sum(transformed_values + transformed_adv, axis=1)\n",
    "\n",
    "        return joint_q_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb1e4c-b755-4db7-b0c4-de90cb79c3b6",
   "metadata": {},
   "source": [
    "## Compiling the QPLEX and Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "vZCEdiSs-rWX",
   "metadata": {
    "id": "vZCEdiSs-rWX"
   },
   "outputs": [],
   "source": [
    "class QPLEXBuild:\n",
    "    def __init__(self, num_agents, num_states, num_actions, transition_matrices, costs, lr=0.01, memory_length=1000, hidden_dim=64, gamma=0.95):\n",
    "        # initialize the parameters\n",
    "        self.env = MultiAgentEnvironment(num_agents=num_agents, transition_matrices=transition_matrices, costs=costs, num_states=num_states)\n",
    "        self.buffers = [AgentMemory(max_length = memory_length) for agent in range(num_agents)]\n",
    "        self.qplex_model = MixingNetwork(num_states=num_states, num_actions=num_actions, num_agents=num_agents, hidden_dim=hidden_dim)\n",
    "        self.target_qplex_model = MixingNetwork(num_states=num_states, num_actions=num_actions, num_agents=num_agents, hidden_dim=hidden_dim)\n",
    "\n",
    "        # initialize training parameters\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.num_agents = num_agents\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # train_step_counter for updating target network\n",
    "        self.train_step_counter = 0\n",
    "\n",
    "    def choose_action(self, agent_index, state, epsilon):\n",
    "        # choose action based on epsilon-greedy probability\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            # onehot the received agent state\n",
    "            state_onehot = tf.expand_dims(tf.one_hot(state, self.num_states), axis=0)\n",
    "            # predict q_values\n",
    "            q_values = self.target_qplex_model.agents_dqns[agent_index](state_onehot)\n",
    "\n",
    "            # select action with minimum q_value\n",
    "            return tf.argmin(q_values, axis=1).numpy()[0]  # note that this is for a single agent\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_qplex_model.set_weights(self.qplex_model.get_weights())\n",
    "\n",
    "    def train_step(self, states, actions, costs, next_states, global_states, global_next_states, batch_size, use_target_network, target_update_period, done = False):\n",
    "        # initialize the loss\n",
    "        mse_ = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # obtaint the current q values\n",
    "            current_joint_q = self.qplex_model(states=states, actions=actions, global_states=global_states, batch_size=batch_size) # batch_size, 1\n",
    "\n",
    "            # calculate the target q values\n",
    "            target_model = self.target_qplex_model if use_target_network else self.qplex_model\n",
    "            #target_q_values = target_model(states=states, actions=actions, global_states=global_states, batch_size=batch_size).numpy() # this is y\n",
    "\n",
    "            # calculate the backup q\n",
    "            backup_joint_q = target_model(states=next_states, actions=actions, global_states=global_next_states,\n",
    "                                          batch_size=batch_size, target_network=True).numpy() # batch_size, 1\n",
    "\n",
    "            # calculate the target q with bellman equation\n",
    "            target_joint_q = tf.convert_to_tensor(costs + self.gamma * backup_joint_q * (1-done))\n",
    "\n",
    "            loss = mse_(target_joint_q, current_joint_q)\n",
    "\n",
    "        # compute the gradient\n",
    "        grads = tape.gradient(loss, self.qplex_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.qplex_model.trainable_variables))\n",
    "\n",
    "        # update the target network counted\n",
    "        self.train_step_counter += 1\n",
    "        if use_target_network and target_update_period and self.train_step_counter % target_update_period == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, episodes, steps_per_episode, training_cost=1e5, batch_size=32, epsilon=1, epsilon_min=0.01, epsilon_decay=0.9995, episodic=False, terminal_period = False,\n",
    "             use_target_network = False, target_update_period = None):\n",
    "        # initialize epsilon for all agents\n",
    "        epsilon_per_agent = [epsilon for agent in range(self.num_agents)]\n",
    "        # initialize old policy - do this later\n",
    "        previous_policy_per_agent = np.random.rand(self.num_agents, self.num_states)\n",
    "\n",
    "        training_cost = [training_cost for agent in range(self.num_agents)]\n",
    "        total_cost = [0 for agent in range(self.num_agents)]\n",
    "\n",
    "        # initialize state in case it is not episodic\n",
    "        states = self.env.reset()\n",
    "        losses = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            terminal_period_counter = 0\n",
    "\n",
    "            for step_ in range(steps_per_episode):\n",
    "                terminal_period_counter += 1\n",
    "\n",
    "                # each agent chooses an action\n",
    "                actions = [self.choose_action(agent_index=agent, state=states[agent], epsilon=epsilon_per_agent[agent]) for agent in range(self.num_agents)] # shape: num_agents, 1\n",
    "\n",
    "                # obtain the one step transition information\n",
    "                next_states, costs, _ = self.env.step(states=states, actions=actions) # no need for the dones per agent. shape for each one (num_agents, 1)\n",
    "\n",
    "                # store in memory\n",
    "                for agent_ in range(self.num_agents):\n",
    "                    self.buffers[agent_].add((states[agent_], actions[agent_], next_states[agent_], costs[agent_], _[agent_]))\n",
    "\n",
    "                # train if enough samples\n",
    "                if len(self.buffers[0]) >= batch_size:\n",
    "                    # sample experiences\n",
    "                    batch_info = [buffer.sample(batch_size, num_states=self.num_states) for buffer in self.buffers] # shape (num_agents, batch_size)\n",
    "\n",
    "                    # obtain batch information\n",
    "                    batch_states = np.array([data[0] for data in batch_info]) # num_agents, batch_size, [1]\n",
    "                    batch_actions = np.array([data[1] for data in batch_info])\n",
    "                    batch_next_states = np.array([data[2] for data in batch_info]) # num_agents, batch_size, [1]\n",
    "                    batch_costs_ = [data[3] for data in batch_info]\n",
    "\n",
    "                    # calculate the joint cost\n",
    "                    joint_bacth_costs = tf.expand_dims(tf.reduce_sum(tf.stack(batch_costs_, axis=1), axis=1), axis=-1)\n",
    "\n",
    "                    # create the global_current_state\n",
    "                    batch_global_state = tf.stack(batch_states.tolist(), axis=1) # batch_size, num_agents\n",
    "                    batch_global_next_state = tf.stack(batch_next_states.tolist(), axis=1) # batch_size, num_agents\n",
    "\n",
    "                    # reshape the global and global next states so that its batch size, num_states*2\n",
    "                    batch_global_state = tf.reshape(batch_global_state, (batch_global_state.shape[0], -1))\n",
    "                    batch_global_next_state = tf.reshape(batch_global_next_state, (batch_global_next_state.shape[0], -1))\n",
    "\n",
    "                    # update the dones if episodic\n",
    "                    done = False\n",
    "                    if terminal_period and terminal_period_counter % terminal_period == 0:\n",
    "                        done = True\n",
    "                    if terminal_period == False and episode == episodes-1 and step_ == steps_per_episode:\n",
    "                        done = True\n",
    "\n",
    "                    # calculate the loss\n",
    "                    loss = self.train_step(states =batch_states, actions=batch_actions, next_states=batch_next_states, costs=joint_bacth_costs,\n",
    "                                          global_states = batch_global_state, global_next_states=batch_global_next_state, batch_size=batch_size,\n",
    "                                          use_target_network=use_target_network, target_update_period=target_update_period, done=done)\n",
    "                    losses.append(loss)\n",
    "                    #print(f'Running: {loss}')\n",
    "\n",
    "                # update states\n",
    "                states = next_states\n",
    "                # update training cost, total_cost, epsilon\n",
    "                for ag_ in range(self.num_agents):\n",
    "                    training_cost[ag_] -= costs[ag_]\n",
    "                    total_cost[ag_] += costs[ag_]\n",
    "                    epsilon_per_agent[ag_] = max(epsilon_min, epsilon_per_agent[ag_]*epsilon_decay)\n",
    "\n",
    "            # check for convergence\n",
    "            # obtain current policy\n",
    "            current_policy = self.get_policy()\n",
    "            #print('Current Policy for All Agents \\n{}\\n'.format(current_policy))\n",
    "            if np.all(current_policy == previous_policy_per_agent):\n",
    "                print(f'Convergence Reached With Statble Policy at {episode}')\n",
    "                print(f'Optimal Policy: {current_policy}')\n",
    "                break\n",
    "            else:\n",
    "                previous_policy_per_agent = current_policy\n",
    "\n",
    "            # reset the starting state if it is the case the the system resets every year\n",
    "            if episodic:\n",
    "                state = self.env.reset()\n",
    "\n",
    "\n",
    "    def get_policy(self):\n",
    "        # get the policy for each agent\n",
    "        id_vector = np.eye(self.num_states)\n",
    "        q_values_agents = []\n",
    "\n",
    "        for agent in range(self.num_agents):\n",
    "            q_vals = self.qplex_model.agents_dqns[agent](id_vector)\n",
    "            q_vals = np.argmin(q_vals, axis=1)\n",
    "            q_values_agents.append(q_vals)\n",
    "\n",
    "        return np.array(q_values_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tf0KWpZS-sWX",
   "metadata": {
    "id": "tf0KWpZS-sWX"
   },
   "source": [
    "## Testing the QPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "hsLn9Xgi-maS",
   "metadata": {
    "id": "hsLn9Xgi-maS"
   },
   "outputs": [],
   "source": [
    "def unpack_data(transition_matrix, cost, num_actions):\n",
    "    # convert all\n",
    "    for agent in range(len(transition_matrix)):\n",
    "        transition_matrix[agent] = {int(k):v for k, v in transition_matrix[agent].items()}\n",
    "        cost[agent] = {int(k) : v for k, v in cost[agent].items()}\n",
    "\n",
    "        for action in range(num_actions):\n",
    "            transition_matrix[agent][action] = pd.DataFrame(json.loads(transition_matrix[agent][action]))\n",
    "            # obtain the columns\n",
    "            agent_action_cols = [float(i) for i in transition_matrix[agent][action].columns.tolist()]\n",
    "            transition_matrix[agent][action].columns = agent_action_cols\n",
    "            cost[agent][action] = {int(k) : v for k, v in cost[agent][action].items()}\n",
    "\n",
    "    return transition_matrix, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "uA2gIhLC-1_I",
   "metadata": {
    "id": "uA2gIhLC-1_I"
   },
   "outputs": [],
   "source": [
    "file_ = 'multiagent_historical_information.json'\n",
    "\n",
    "with open(file_, 'r') as file:\n",
    "  agents_information = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "FF0HJrJI-6jT",
   "metadata": {
    "id": "FF0HJrJI-6jT"
   },
   "outputs": [],
   "source": [
    "agent_id, transition_matrices, cost = agents_information\n",
    "\n",
    "transition_matrices, costs = unpack_data(transition_matrix=transition_matrices, cost=cost, num_actions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "gm4JMtJY_G4W",
   "metadata": {
    "id": "gm4JMtJY_G4W"
   },
   "outputs": [],
   "source": [
    "idx_sample_agent = [0, 3]\n",
    "\n",
    "trans_matrices_selected = [transition_matrices[i] for i in idx_sample_agent]\n",
    "costs_selected = [costs[i] for i in idx_sample_agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "R9i9ybpp_KAU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9i9ybpp_KAU",
    "outputId": "0686dfac-7e0c-489b-b8eb-de06c6d6b28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence Reached With Statble Policy at 2\n",
      "Optimal Policy: [[0 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# initialize the agent\n",
    "agents_qplex = QPLEXBuild(num_agents=2, num_states=3, num_actions=2, transition_matrices=trans_matrices_selected,\n",
    "                          costs=costs_selected)\n",
    "\n",
    "# train the model\n",
    "agents_qplex.train(episodes=5, steps_per_episode=365, use_target_network=True, target_update_period=10, epsilon_decay=0.995, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
